# üìö T√†i Li·ªáu H·ªá Th·ªëng - Web Scraper Extension

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan](#t·ªïng-quan)
2. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
3. [C√°c Th√†nh Ph·∫ßn Ch√≠nh](#c√°c-th√†nh-ph·∫ßn-ch√≠nh)
4. [Ch·ª©c NƒÉng Chi Ti·∫øt](#ch·ª©c-nƒÉng-chi-ti·∫øt)
5. [C√†i ƒê·∫∑t v√† C·∫•u H√¨nh](#c√†i-ƒë·∫∑t-v√†-c·∫•u-h√¨nh)
6. [H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng](#h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)
7. [API Documentation](#api-documentation)
8. [X·ª≠ L√Ω D·ªØ Li·ªáu](#x·ª≠-l√Ω-d·ªØ-li·ªáu)
9. [Troubleshooting](#troubleshooting)

---

## üéØ T·ªïng Quan 

**Web Scraper Extension** l√† m·ªôt h·ªá th·ªëng c√†o d·ªØ li·ªáu web to√†n di·ªán, bao g·ªìm:

- **Browser Extension** (Chrome/Edge): Giao di·ªán ng∆∞·ªùi d√πng ƒë·ªÉ ch·ªçn v√† c√†o d·ªØ li·ªáu
- **API Server** (Python): X·ª≠ l√Ω logic c√†o d·ªØ li·ªáu s·ª≠ d·ª•ng Crawl4AI
- **Web Scraper**: Th∆∞ vi·ªán wrapper cho Crawl4AI v·ªõi nhi·ªÅu t√≠nh nƒÉng n√¢ng cao

### T√≠nh NƒÉng Ch√≠nh

‚úÖ **Click-to-Select**: Click v√†o ph·∫ßn t·ª≠ tr√™n trang ƒë·ªÉ ch·ªçn tr∆∞·ªùng c·∫ßn l·∫•y  
‚úÖ **Smart Selector Generation**: T·ª± ƒë·ªông t·∫°o CSS selector v√† XPath t·ªëi ∆∞u  
‚úÖ **Dual Scraping Mode**: C√†o b·∫±ng JavaScript (nhanh) ho·∫∑c Crawl4AI (m·∫°nh m·∫Ω)  
‚úÖ **Template System**: L∆∞u v√† t√°i s·ª≠ d·ª•ng template c√†o d·ªØ li·ªáu  
‚úÖ **Lazy Loading Support**: H·ªó tr·ª£ l·∫•y ·∫£nh t·ª´ `data-src` (lazy loading)  
‚úÖ **Image Filtering**: T·ª± ƒë·ªông lo·∫°i b·ªè SVG, placeholder, v√† binary data  
‚úÖ **Container Extraction**: L·∫•y to√†n b·ªô gi√° tr·ªã trong container v·ªõi `itemprop`  
‚úÖ **Multiple Value Types**: H·ªó tr·ª£ text, HTML, attributes (src, href, alt, etc.)  
‚úÖ **Export JSON**: Xu·∫•t d·ªØ li·ªáu ƒë√£ c√†o th√†nh file JSON  

---

## üíª C√¥ng Ngh·ªá v√† Phi√™n B·∫£n

### Ng√¥n Ng·ªØ L·∫≠p Tr√¨nh

#### Python
- **Phi√™n b·∫£n**: Python 3.10.6
- **M√¥ t·∫£**: Ng√¥n ng·ªØ l·∫≠p tr√¨nh ch√≠nh cho backend, API server, v√† c√°c c√¥ng c·ª• scraping
- **S·ª≠ d·ª•ng cho**: 
  - Streamlit Dashboard (`dashboard.py`)
  - Listing Crawler (`listing_crawler.py`)
  - Web Scraper (`web_scraper.py`)
  - Database Handler (`database.py`)
  - Extension API Server (`extension_api_server.py`)

### Framework v√† Th∆∞ Vi·ªán Python

#### 1. Streamlit
- **Phi√™n b·∫£n**: 1.52.1 (y√™u c·∫ßu >= 1.28.0)
- **M√¥ t·∫£**: Framework ƒë·ªÉ x√¢y d·ª±ng web dashboard t∆∞∆°ng t√°c
- **S·ª≠ d·ª•ng cho**: 
  - Dashboard ch√≠nh (`dashboard.py`) - giao di·ªán qu·∫£n l√Ω scraping tasks
  - Hi·ªÉn th·ªã k·∫øt qu·∫£ scraping
  - Qu·∫£n l√Ω templates v√† c·∫•u h√¨nh

#### 2. Crawl4AI
- **Phi√™n b·∫£n**: 0.7.7 (y√™u c·∫ßu >= 0.4.0)
- **M√¥ t·∫£**: Framework web scraping m·∫°nh m·∫Ω v·ªõi h·ªó tr·ª£ JavaScript rendering
- **S·ª≠ d·ª•ng cho**: 
  - Crawl v√† extract d·ªØ li·ªáu t·ª´ c√°c trang web
  - H·ªó tr·ª£ CSS selector v√† XPath
  - X·ª≠ l√Ω lazy loading v√† dynamic content
  - File: `web_scraper.py`, `listing_crawler.py`

#### 3. Nodriver (undetected-chromedriver)
- **Phi√™n b·∫£n**: 0.48.1
- **M√¥ t·∫£**: Th∆∞ vi·ªán browser automation ƒë·ªÉ tr√°nh bot detection
- **S·ª≠ d·ª•ng cho**: 
  - Crawl listing pages v·ªõi kh·∫£ nƒÉng tr√°nh ph√°t hi·ªán bot
  - Scroll v√† lazy load content
  - File: `listing_crawler.py`, `dashboard.py`

#### 4. Playwright
- **Phi√™n b·∫£n**: 1.56.0
- **M√¥ t·∫£**: Browser automation framework (ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi Crawl4AI)
- **S·ª≠ d·ª•ng cho**: 
  - Crawl4AI s·ª≠ d·ª•ng Playwright l√†m engine
  - H·ªó tr·ª£ headless browser automation

#### 5. BeautifulSoup4
- **Phi√™n b·∫£n**: 4.14.3 (y√™u c·∫ßu >= 4.12.0)
- **M√¥ t·∫£**: Th∆∞ vi·ªán parsing HTML/XML
- **S·ª≠ d·ª•ng cho**: 
  - Parse v√† x·ª≠ l√Ω HTML content
  - Extract d·ªØ li·ªáu t·ª´ DOM

#### 6. Pandas
- **Phi√™n b·∫£n**: 2.3.3 (y√™u c·∫ßu >= 2.0.0)
- **M√¥ t·∫£**: Th∆∞ vi·ªán ph√¢n t√≠ch v√† x·ª≠ l√Ω d·ªØ li·ªáu
- **S·ª≠ d·ª•ng cho**: 
  - X·ª≠ l√Ω v√† hi·ªÉn th·ªã d·ªØ li·ªáu scraping trong dashboard
  - Export d·ªØ li·ªáu ra Excel/CSV

#### 7. OpenPyXL
- **Phi√™n b·∫£n**: >= 3.1.0 (trong requirements.txt)
- **M√¥ t·∫£**: Th∆∞ vi·ªán ƒë·ªçc/ghi file Excel (.xlsx)
- **S·ª≠ d·ª•ng cho**: 
  - Export k·∫øt qu·∫£ scraping ra file Excel

#### 8. MySQL Connector
- **Phi√™n b·∫£n**: 
  - `mysql-connector-python`: 9.5.0
  - `pymysql`: (fallback option)
- **M√¥ t·∫£**: Th∆∞ vi·ªán k·∫øt n·ªëi v√† t∆∞∆°ng t√°c v·ªõi MySQL database
- **S·ª≠ d·ª•ng cho**: 
  - L∆∞u tr·ªØ collected links trong database
  - Qu·∫£n l√Ω tr·∫°ng th√°i scraping (PENDING, PROCESSED, ERROR)
  - File: `database.py`

#### 9. Python-dotenv
- **Phi√™n b·∫£n**: (trong requirements.txt)
- **M√¥ t·∫£**: Th∆∞ vi·ªán qu·∫£n l√Ω bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file `.env`
- **S·ª≠ d·ª•ng cho**: 
  - C·∫•u h√¨nh database connection
  - Qu·∫£n l√Ω API keys v√† secrets

#### 10. TF-Playwright-Stealth
- **Phi√™n b·∫£n**: 1.2.0
- **M√¥ t·∫£**: Plugin ƒë·ªÉ tr√°nh bot detection cho Playwright
- **S·ª≠ d·ª•ng cho**: 
  - TƒÉng kh·∫£ nƒÉng tr√°nh ph√°t hi·ªán khi scraping

### Database

#### MySQL
- **Phi√™n b·∫£n**: (t√πy theo c√†i ƒë·∫∑t Laragon)
- **M√¥ t·∫£**: H·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu quan h·ªá
- **C·∫•u h√¨nh m·∫∑c ƒë·ªãnh**:
  - Host: `localhost`
  - User: `root`
  - Password: `` (empty)
  - Database: `craw_db`
- **B·∫£ng ch√≠nh**:
  - `collected_links`: L∆∞u tr·ªØ c√°c link ƒë√£ thu th·∫≠p
    - `id`: INT AUTO_INCREMENT PRIMARY KEY
    - `url`: VARCHAR(2000) UNIQUE
    - `status`: VARCHAR(50) DEFAULT 'PENDING'
    - `created_at`: TIMESTAMP

### Browser Extension (Chrome/Edge)

#### Manifest Version
- **Manifest V3**: Phi√™n b·∫£n m·ªõi nh·∫•t c·ªßa Chrome Extension API
- **Extension Version**: 1.0.9

#### C√¥ng Ngh·ªá Frontend

##### JavaScript (Vanilla)
- **M√¥ t·∫£**: Kh√¥ng s·ª≠ d·ª•ng framework, pure JavaScript
- **S·ª≠ d·ª•ng cho**: 
  - `content.js`: Content script ch·∫°y tr√™n trang web
  - `sidepanel.js`: Logic UI cho side panel
  - `background.js`: Service worker

##### HTML5
- **S·ª≠ d·ª•ng cho**: 
  - `sidepanel.html`: Giao di·ªán side panel
  - `manifest.json`: C·∫•u h√¨nh extension

##### CSS3
- **S·ª≠ d·ª•ng cho**: 
  - `sidepanel.css`: Styling cho side panel
  - `content.css`: Styling cho content script (highlight, overlay)

#### Chrome Extension APIs
- **activeTab**: Truy c·∫≠p tab hi·ªán t·∫°i
- **storage**: L∆∞u tr·ªØ templates v√† c·∫•u h√¨nh
- **scripting**: Inject scripts v√†o trang
- **tabs**: Qu·∫£n l√Ω tabs
- **sidePanel**: Hi·ªÉn th·ªã side panel

### H·ªá ƒêi·ªÅu H√†nh v√† M√¥i Tr∆∞·ªùng

#### H·ªá ƒêi·ªÅu H√†nh
- **Windows**: 10.0.19045 (Windows 10/11)
- **Shell**: PowerShell

#### Development Environment
- **Laragon**: Local development environment
  - MySQL server
  - PHP (n·∫øu c·∫ßn)
  - Python environment

### C√¥ng C·ª• v√† Utilities

#### URL Parsing
- **urllib.parse**: Module Python chu·∫©n
  - `urlparse`, `urlunparse`, `parse_qs`, `urlencode`
  - S·ª≠ d·ª•ng cho: Normalize URLs, parse query parameters

#### Path Handling
- **pathlib**: Module Python chu·∫©n
  - S·ª≠ d·ª•ng cho: Qu·∫£n l√Ω file paths, cross-platform compatibility

#### Async/Await
- **asyncio**: Module Python chu·∫©n
  - S·ª≠ d·ª•ng cho: X·ª≠ l√Ω asynchronous operations
  - Windows-specific: `WindowsProactorEventLoopPolicy`

#### JSON Processing
- **json**: Module Python chu·∫©n
  - S·ª≠ d·ª•ng cho: Serialize/deserialize templates v√† d·ªØ li·ªáu

#### DateTime
- **datetime**: Module Python chu·∫©n
  - S·ª≠ d·ª•ng cho: Timestamp, logging

### C·∫•u H√¨nh Browser (Nodriver)

#### Performance Optimization
```python
BROWSER_CONFIG_TIET_KIEM = [
    "--blink-settings=imagesEnabled=false", 
    "--disable-images",
    "--mute-audio",
]
```
- **M·ª•c ƒë√≠ch**: Gi·∫£m lag v√† ti·∫øt ki·ªám bandwidth
- **T√°c d·ª•ng**: 
  - Ch·∫∑n t·∫£i ·∫£nh
  - T·∫Øt audio
  - TƒÉng t·ªëc ƒë·ªô crawl

### T√≥m T·∫Øt Phi√™n B·∫£n

| C√¥ng Ngh·ªá | Phi√™n B·∫£n | Vai Tr√≤ v√† M·ª•c ƒê√≠ch S·ª≠ D·ª•ng |
|-----------|-----------|----------------------------|
| **Python** | 3.10.6 | **Ng√¥n ng·ªØ l·∫≠p tr√¨nh ch√≠nh** - Vi·∫øt to√†n b·ªô backend, API server, crawler, v√† dashboard. H·ªó tr·ª£ async/await cho x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô. |
| **Streamlit** | 1.52.1 | **Web Dashboard Framework** - T·∫°o giao di·ªán web t∆∞∆°ng t√°c ƒë·ªÉ qu·∫£n l√Ω scraping tasks, xem k·∫øt qu·∫£, c·∫•u h√¨nh templates. File: `dashboard.py` |
| **Crawl4AI** | 0.7.7 | **Web Scraping Framework ch√≠nh** - Crawl v√† extract d·ªØ li·ªáu t·ª´ websites v·ªõi h·ªó tr·ª£ JavaScript rendering, CSS selector, XPath. File: `web_scraper.py`, `listing_crawler.py` |
| **Nodriver** | 0.48.1 | **Browser Automation v·ªõi Anti-Detection** - Tr√°nh bot detection khi crawl listing pages, scroll v√† lazy load content. File: `listing_crawler.py`, `dashboard.py` |
| **Playwright** | 1.56.0 | **Browser Engine** - ƒê∆∞·ª£c Crawl4AI s·ª≠ d·ª•ng l√†m engine ƒë·ªÉ ƒëi·ªÅu khi·ªÉn browser (headless/headful). T·ª± ƒë·ªông c√†i khi c√†i Crawl4AI. |
| **BeautifulSoup4** | 4.14.3 | **HTML Parsing Library** - Parse v√† extract d·ªØ li·ªáu t·ª´ HTML ƒë√£ crawl. S·ª≠ d·ª•ng trong `extract_batdongsan.py` ƒë·ªÉ parse HTML v√† t√¨m c√°c th·∫ª, attributes, text content. |
| **Pandas** | 2.3.3 | **Data Processing & Display** - X·ª≠ l√Ω v√† hi·ªÉn th·ªã d·ªØ li·ªáu scraping d∆∞·ªõi d·∫°ng b·∫£ng (DataFrame) trong dashboard. Export ra Excel/CSV. File: `dashboard.py` (hi·ªÉn th·ªã k·∫øt qu·∫£, collected links) |
| **MySQL Connector** | 9.5.0 | **Database Connection** - K·∫øt n·ªëi v√† t∆∞∆°ng t√°c v·ªõi MySQL database ƒë·ªÉ l∆∞u collected links, qu·∫£n l√Ω tr·∫°ng th√°i scraping (PENDING, CRAWLED, ERROR). File: `database.py` |
| **Chrome Extension** | Manifest V3 | **Browser Extension Platform** - N·ªÅn t·∫£ng ƒë·ªÉ x√¢y d·ª±ng extension cho Chrome/Edge. Manifest V3 l√† phi√™n b·∫£n m·ªõi nh·∫•t c·ªßa Chrome Extension API v·ªõi service worker. |
| **Extension Version** | 1.0.9 | **Phi√™n b·∫£n Extension** - Version hi·ªán t·∫°i c·ªßa extension, ƒë∆∞·ª£c khai b√°o trong `manifest.json`. D√πng ƒë·ªÉ qu·∫£n l√Ω updates v√† compatibility. |

### Gi·∫£i Th√≠ch Chi Ti·∫øt Vai Tr√≤

#### BeautifulSoup4 (4.14.3) - HTML Parsing
**Vai tr√≤ trong d·ª± √°n:**
- **Parse HTML**: Sau khi Crawl4AI crawl ƒë∆∞·ª£c HTML t·ª´ website, BeautifulSoup4 ƒë∆∞·ª£c d√πng ƒë·ªÉ parse HTML th√†nh c·∫•u tr√∫c DOM c√≥ th·ªÉ truy v·∫•n
- **Extract d·ªØ li·ªáu**: T√¨m v√† extract c√°c th·∫ª HTML, attributes, text content d·ª±a tr√™n CSS selector ho·∫∑c th·∫ª HTML
- **File s·ª≠ d·ª•ng**: `extract_batdongsan.py`
  - Parse HTML ƒë·ªÉ t√¨m title, ƒë·ªãa ch·ªâ, gi√°, m√¥ t·∫£, h√¨nh ·∫£nh
  - T√¨m c√°c th·∫ª nh∆∞ `<h1>`, `<span>`, `<strong>` v·ªõi c√°c class/id c·ª• th·ªÉ
  - Extract attributes nh∆∞ `itemprop`, `src`, `href`

**V√≠ d·ª• s·ª≠ d·ª•ng:**
```python
soup = BeautifulSoup(html, 'html.parser')
h1 = soup.find('h1')  # T√¨m th·∫ª h1
title = h1.get_text()  # L·∫•y text content
```

#### Pandas (2.3.3) - Data Processing & Display
**Vai tr√≤ trong d·ª± √°n:**
- **Hi·ªÉn th·ªã d·ªØ li·ªáu d·∫°ng b·∫£ng**: Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ scraping (list of dicts) th√†nh DataFrame ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit dashboard
- **X·ª≠ l√Ω d·ªØ li·ªáu**: Filter, sort, group d·ªØ li·ªáu scraping
- **Export d·ªØ li·ªáu**: Xu·∫•t k·∫øt qu·∫£ ra file Excel (.xlsx) ho·∫∑c CSV
- **File s·ª≠ d·ª•ng**: `dashboard.py`
  - Hi·ªÉn th·ªã collected links trong Tab 3
  - Hi·ªÉn th·ªã k·∫øt qu·∫£ scraping trong Tab 2
  - Export results ra Excel/CSV

**V√≠ d·ª• s·ª≠ d·ª•ng:**
```python
df = pd.DataFrame(recent_links)  # Chuy·ªÉn list th√†nh DataFrame
df['created_at'] = pd.to_datetime(df['created_at'])  # Convert datetime
st.dataframe(df)  # Hi·ªÉn th·ªã trong Streamlit
df.to_excel('results.xlsx')  # Export ra Excel
```

#### Chrome Extension (Manifest V3) - Browser Extension Platform
**Vai tr√≤ trong d·ª± √°n:**
- **N·ªÅn t·∫£ng extension**: Manifest V3 l√† phi√™n b·∫£n m·ªõi nh·∫•t c·ªßa Chrome Extension API
- **Service Worker**: Thay th·∫ø background page b·∫±ng service worker (ch·∫°y n·ªÅn)
- **Side Panel API**: H·ªó tr·ª£ side panel m·ªõi (thay v√¨ popup)
- **B·∫£o m·∫≠t**: TƒÉng c∆∞·ªùng b·∫£o m·∫≠t v·ªõi Content Security Policy (CSP) nghi√™m ng·∫∑t h∆°n
- **File**: `extension/manifest.json`

**T√≠nh nƒÉng Manifest V3:**
- `activeTab`: Truy c·∫≠p tab hi·ªán t·∫°i khi user click extension
- `storage`: L∆∞u templates v√† c·∫•u h√¨nh
- `scripting`: Inject content script v√†o trang web
- `sidePanel`: Hi·ªÉn th·ªã side panel UI

#### Extension Version (1.0.9) - Version Management
**Vai tr√≤ trong d·ª± √°n:**
- **Version control**: Qu·∫£n l√Ω phi√™n b·∫£n extension ƒë·ªÉ theo d√µi updates
- **Compatibility**: ƒê·∫£m b·∫£o extension t∆∞∆°ng th√≠ch v·ªõi c√°c version kh√°c nhau
- **Updates**: Chrome t·ª± ƒë·ªông ki·ªÉm tra v√† th√¥ng b√°o khi c√≥ version m·ªõi
- **File**: `extension/manifest.json` - field `"version": "1.0.9"`

**C√°ch ho·∫°t ƒë·ªông:**
- Khi user c√†i extension, Chrome l∆∞u version n√†y
- Khi c√≥ update, Chrome so s√°nh version m·ªõi v·ªõi version c≈©
- Extension c√≥ th·ªÉ t·ª± ƒë·ªông update ho·∫∑c y√™u c·∫ßu user reload

### Y√™u C·∫ßu H·ªá Th·ªëng

#### Python
- Python >= 3.10
- pip (package manager)

#### Database
- MySQL Server (qua Laragon ho·∫∑c standalone)
- Quy·ªÅn t·∫°o database v√† tables

#### Browser
- Google Chrome ho·∫∑c Microsoft Edge (phi√™n b·∫£n m·ªõi nh·∫•t)
- ƒê·ªÉ s·ª≠ d·ª•ng extension

#### Network
- K·∫øt n·ªëi Internet ƒë·ªÉ crawl websites
- Proxy (t√πy ch·ªçn) ƒë·ªÉ tr√°nh rate limiting

---

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Browser Extension                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Side Panel  ‚îÇ  ‚îÇ Content Script‚îÇ  ‚îÇ Background   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (UI)        ‚îÇ  ‚îÇ (Selection)   ‚îÇ  ‚îÇ (Service)    ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                            ‚îÇ                                   ‚îÇ
‚îÇ                            ‚ñº                                   ‚îÇ
‚îÇ                    HTTP POST Request                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Extension API Server (Python)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ExtensionAPIHandler                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - handle_scrape_with_template()                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - handle_scrape_with_fields()                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                     ‚îÇ                                         ‚îÇ
‚îÇ                     ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  WebScraper (Wrapper)                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - scrape_with_schema()                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - scrape_simple()                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                     ‚îÇ                                         ‚îÇ
‚îÇ                     ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Crawl4AI                                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - AsyncWebCrawler                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - JsonCssExtractionStrategy                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ C√°c Th√†nh Ph·∫ßn Ch√≠nh

### 1. Browser Extension (`extension/`)

#### 1.1. Manifest (`manifest.json`)
- **Version**: 1.0.9
- **Permissions**: `activeTab`, `storage`, `scripting`, `tabs`, `sidePanel`
- **Content Scripts**: Ch·∫°y tr√™n t·∫•t c·∫£ c√°c trang web (tr·ª´ localhost, streamlit)
- **Side Panel**: Giao di·ªán ch√≠nh c·ªßa extension

#### 1.2. Content Script (`content.js`)
**Ch·ª©c nƒÉng:**
- X·ª≠ l√Ω click ƒë·ªÉ ch·ªçn ph·∫ßn t·ª≠ tr√™n trang
- T·∫°o CSS selector v√† XPath t·ª± ƒë·ªông
- Highlight c√°c ph·∫ßn t·ª≠ ƒë√£ ch·ªçn
- Extract d·ªØ li·ªáu t·ª´ DOM (client-side scraping)
- T√¨m label/description cho c√°c tr∆∞·ªùng (sibling strategy, uncle strategy)

**T√≠nh nƒÉng n·ªïi b·∫≠t:**
- **Smart Selector Generation**: 
  - ∆Øu ti√™n ID, data attributes
  - S·ª≠ d·ª•ng `itemprop` attribute cho XPath ch√≠nh x√°c
  - T√¨m label t·ª´ sibling ho·∫∑c "uncle" element (div.a4ep88f)
  - Fallback v·ªÅ absolute XPath n·∫øu c·∫ßn
- **Keyboard Shortcut**: Nh·∫•n `X` ƒë·ªÉ toggle gi·ªØa CSS v√† XPath
- **Preview Value**: Hi·ªÉn th·ªã preview gi√° tr·ªã tr∆∞·ªõc khi crawl

#### 1.3. Side Panel (`sidepanel.js`, `sidepanel.html`, `sidepanel.css`)
**Ch·ª©c nƒÉng:**
- Giao di·ªán qu·∫£n l√Ω c√°c tr∆∞·ªùng ƒë√£ ch·ªçn
- B·∫≠t/t·∫Øt ch·∫ø ƒë·ªô ch·ªçn ph·∫ßn t·ª≠
- Ch·ªçn lo·∫°i selector (CSS/XPath)
- Ch·ªçn lo·∫°i gi√° tr·ªã (text, html, src, href, all, etc.)
- Preview gi√° tr·ªã ƒë√£ ch·ªçn
- C√†o d·ªØ li·ªáu (JavaScript ho·∫∑c Crawl4AI)
- Export JSON
- L∆∞u/Load template

**UI Components:**
- Fields list v·ªõi min-height 100px
- Action buttons (C√†o, Export, L∆∞u Template)
- Selector type toggle (CSS/XPath)
- Value type dropdown (text, html, src, href, all, etc.)

#### 1.4. Background Script (`background.js`)
- Service worker cho extension
- X·ª≠ l√Ω c√°c s·ª± ki·ªán extension
- Qu·∫£n l√Ω side panel

### 2. API Server (`extension_api_server.py`)

**Port m·∫∑c ƒë·ªãnh**: `8765`

**Endpoints:**
- `GET /`: Health check
- `POST /`: X·ª≠ l√Ω c√°c action t·ª´ extension

**Actions h·ªó tr·ª£:**
1. `scrape_with_template`: C√†o d·ªØ li·ªáu s·ª≠ d·ª•ng template ƒë√£ l∆∞u
2. `scrape_with_fields`: C√†o d·ªØ li·ªáu v·ªõi c√°c tr∆∞·ªùng ƒë∆∞·ª£c ch·ªçn tr·ª±c ti·∫øp

**T√≠nh nƒÉng x·ª≠ l√Ω:**
- Chuy·ªÉn ƒë·ªïi selector t·ª´ extension format sang Crawl4AI schema
- X·ª≠ l√Ω `valueType: 'all'` (container extraction v·ªõi `itemprop`)
- X·ª≠ l√Ω `valueType: 'src'` (lazy loading images)
- Filter binary data, SVG, placeholder images
- Debug logging chi ti·∫øt

### 3. Web Scraper (`web_scraper.py`)

**Class**: `WebScraper`

**Methods:**
- `scrape_simple(url)`: C√†o ƒë∆°n gi·∫£n, l·∫•y to√†n b·ªô n·ªôi dung
- `scrape_with_schema(url, schema)`: C√†o v·ªõi schema ƒë·ªãnh nghƒ©a c√°c tr∆∞·ªùng
- `scrape_with_llm(url, prompt)`: C√†o s·ª≠ d·ª•ng LLM ƒë·ªÉ extract (n·∫øu c·∫•u h√¨nh)

**Features:**
- Async/await support
- Context manager (`async with`)
- Browser configuration (headless, viewport)
- Cache mode support

### 4. Template System

**Template Format:**
```json
{
  "name": "Template Name",
  "url": "https://example.com",
  "createdAt": "2024-01-01T00:00:00Z",
  "baseSelector": "body",
  "fields": [
    {
      "name": "Field Name",
      "selector": ".css-selector",
      "xpath": "//xpath/expression",
      "type": "text",
      "valueType": "text",
      "attribute": null
    }
  ]
}
```

**Scripts h·ªó tr·ª£:**
- `scrape_with_template.py`: Script CLI ƒë·ªÉ c√†o v·ªõi template
- `use_template_example.py`: V√≠ d·ª• s·ª≠ d·ª•ng template

---

## ‚öôÔ∏è Ch·ª©c NƒÉng Chi Ti·∫øt

### 1. Ch·ªçn Ph·∫ßn T·ª≠ (Element Selection)

**C√°ch ho·∫°t ƒë·ªông:**
1. User click "B·∫≠t ch·∫ø ƒë·ªô ch·ªçn" trong side panel
2. Content script b·∫≠t event listener cho click
3. User click v√†o ph·∫ßn t·ª≠ tr√™n trang
4. Content script:
   - Highlight ph·∫ßn t·ª≠ (border m√†u xanh)
   - T·∫°o selector (CSS ho·∫∑c XPath)
   - T√¨m label/description n·∫øu c√≥
   - Th√™m v√†o danh s√°ch fields

**Selector Generation Strategy:**
1. **ID/Data Attributes**: ∆Øu ti√™n cao nh·∫•t
2. **Itemprop Attribute**: Cho XPath ch√≠nh x√°c
3. **Label Sibling**: T√¨m label t·ª´ previous sibling
4. **Uncle Strategy**: T√¨m label t·ª´ div.a4ep88f ho·∫∑c span kh√¥ng trong strong
5. **Container Class**: S·ª≠ d·ª•ng class c·ªßa container
6. **Absolute XPath**: Fallback cu·ªëi c√πng

### 2. Value Types

#### 2.1. `text` (M·∫∑c ƒë·ªãnh)
- L·∫•y text content c·ªßa ph·∫ßn t·ª≠
- Lo·∫°i b·ªè HTML tags
- Trim whitespace

#### 2.2. `html`
- L·∫•y to√†n b·ªô HTML c·ªßa ph·∫ßn t·ª≠
- Gi·ªØ nguy√™n c·∫•u tr√∫c

#### 2.3. `src`
- L·∫•y URL t·ª´ attribute `src` ho·∫∑c `data-src`
- **Lazy Loading Support**: ∆Øu ti√™n `data-src` > `data-lazy-src` > `src`
- **Filter**: Lo·∫°i b·ªè SVG, placeholder, binary data
- **Multiple Images**: S·ª≠ d·ª•ng `type: 'list'` v·ªõi nested fields

#### 2.4. `href`
- L·∫•y URL t·ª´ attribute `href`
- Th∆∞·ªùng d√πng cho links

#### 2.5. `all` / `container`
- L·∫•y to√†n b·ªô gi√° tr·ªã trong container
- T√¨m t·∫•t c·∫£ `strong[@itemprop]` trong container
- Tr·∫£ v·ªÅ dictionary v·ªõi key l√† `itemprop` v√† value l√† text
- Format: `{"house_type": "Nh√† m·∫∑t ph·ªë", "size": "110 m¬≤", ...}`

#### 2.6. C√°c attribute kh√°c
- `alt`, `title`, `data-id`, etc.
- L·∫•y gi√° tr·ªã t·ª´ attribute t∆∞∆°ng ·ª©ng

### 3. Scraping Modes

#### 3.1. JavaScript Scraping (Client-side)
- **N√∫t**: "üöÄ C√†o d·ªØ li·ªáu (JS)"
- **C√°ch ho·∫°t ƒë·ªông**: Extract tr·ª±c ti·∫øp t·ª´ DOM c·ªßa trang hi·ªán t·∫°i
- **∆Øu ƒëi·ªÉm**: Nhanh, kh√¥ng c·∫ßn server
- **Nh∆∞·ª£c ƒëi·ªÉm**: Ch·ªâ ho·∫°t ƒë·ªông tr√™n trang ƒë√£ m·ªü, kh√¥ng h·ªó tr·ª£ JavaScript rendering

#### 3.2. Crawl4AI Scraping (Server-side)
- **N√∫t**: "ü§ñ C√†o v·ªõi Crawl4AI"
- **C√°ch ho·∫°t ƒë·ªông**: 
  1. Extension g·ª≠i request ƒë·∫øn API server
  2. API server t·∫°o schema cho Crawl4AI
  3. Crawl4AI crawl trang web v·ªõi browser automation
  4. Tr·∫£ v·ªÅ k·∫øt qu·∫£ cho extension
- **∆Øu ƒëi·ªÉm**: 
  - H·ªó tr·ª£ JavaScript rendering
  - C√≥ th·ªÉ crawl b·∫•t k·ª≥ URL (kh√¥ng c·∫ßn m·ªü trang)
  - X·ª≠ l√Ω lazy loading, dynamic content
- **Nh∆∞·ª£c ƒëi·ªÉm**: C·∫ßn API server ch·∫°y, ch·∫≠m h∆°n JavaScript scraping

### 4. Image Processing

**Lazy Loading Support:**
- Schema l·∫•y c·∫£ `data-src`, `data-lazy-src`, v√† `src`
- ∆Øu ti√™n: `data-src` > `data-lazy-src` > `src`
- L√Ω do: `data-src` th∆∞·ªùng ch·ª©a URL th·∫≠t, `src` c√≥ th·ªÉ l√† placeholder

**Filtering:**
- **SVG**: Lo·∫°i b·ªè `.svg`, `data:image/svg+xml`
- **Placeholder**: Lo·∫°i b·ªè `img_empty`, `placeholder`, `empty.jpg`, `no-image`, etc.
- **Binary Data**: Lo·∫°i b·ªè base64 images, JFIF, PNG binary
- **URL Only**: Ch·ªâ gi·ªØ l·∫°i URLs (http, https, //, /)

### 5. Container Extraction (`valueType: 'all'`)

**Use Case**: L·∫•y nhi·ªÅu gi√° tr·ªã t·ª´ m·ªôt container, v√≠ d·ª•:
```html
<div class="container">
  <strong itemprop="house_type">Nh√† m·∫∑t ph·ªë</strong>
  <strong itemprop="size">110 m¬≤</strong>
  <strong itemprop="rooms">6 ph√≤ng</strong>
</div>
```

**C√°ch ho·∫°t ƒë·ªông:**
1. T√¨m container selector
2. T√¨m t·∫•t c·∫£ `strong[@itemprop]` trong container
3. Extract text v√† `itemprop` attribute
4. Tr·∫£ v·ªÅ dictionary: `{"house_type": "Nh√† m·∫∑t ph·ªë", "size": "110 m¬≤", ...}`

**Schema cho Crawl4AI:**
```json
{
  "name": "field_name",
  "selector": "//container//strong[@itemprop]",
  "type": "list",
  "fields": [
    {"name": "value", "type": "text"},
    {"name": "itemprop", "type": "attribute", "attribute": "itemprop"}
  ]
}
```

---

## üîß C√†i ƒê·∫∑t v√† C·∫•u H√¨nh

### 1. C√†i ƒê·∫∑t Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies (chi ti·∫øt xem ph·∫ßn [C√¥ng Ngh·ªá v√† Phi√™n B·∫£n](#-c√¥ng-ngh·ªá-v√†-phi√™n-b·∫£n)):**
- `crawl4ai>=0.4.0` (hi·ªán t·∫°i: 0.7.7): Web scraping framework
- `python-dotenv`: Environment variables
- `playwright` (hi·ªán t·∫°i: 1.56.0): Browser automation (t·ª± ƒë·ªông c√†i v·ªõi crawl4ai)
- `beautifulsoup4>=4.12.0` (hi·ªán t·∫°i: 4.14.3): HTML parsing
- `streamlit>=1.28.0` (hi·ªán t·∫°i: 1.52.1): Web dashboard framework
- `pandas>=2.0.0` (hi·ªán t·∫°i: 2.3.3): Data processing
- `openpyxl>=3.1.0`: Excel file handling
- `nodriver`: Browser automation v·ªõi anti-detection (hi·ªán t·∫°i: 0.48.1)
- `mysql-connector-python` ho·∫∑c `pymysql`: MySQL database connection (hi·ªán t·∫°i: 9.5.0)

### 2. C√†i ƒê·∫∑t Extension

1. M·ªü Chrome/Edge: `chrome://extensions/` ho·∫∑c `edge://extensions/`
2. B·∫≠t "Developer mode"
3. Click "Load unpacked"
4. Ch·ªçn th∆∞ m·ª•c `extension/`

### 3. T·∫°o Icons (T√πy ch·ªçn)

```bash
cd extension
pip install Pillow
python generate_icons.py
```

Ho·∫∑c m·ªü `create-icons.html` trong browser v√† download icons.

### 4. Ch·∫°y API Server

```bash
python extension_api_server.py
```

Server s·∫Ω ch·∫°y t·∫°i `http://localhost:8765`

---

## üìñ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng

### 1. C√†o D·ªØ Li·ªáu C∆° B·∫£n

1. **M·ªü trang web** c·∫ßn c√†o
2. **Click icon extension** ƒë·ªÉ m·ªü side panel
3. **Click "B·∫≠t ch·∫ø ƒë·ªô ch·ªçn"**
4. **Click v√†o c√°c ph·∫ßn t·ª≠** mu·ªën l·∫•y d·ªØ li·ªáu
5. **Ch·ªçn value type** cho m·ªói field (text, html, src, etc.)
6. **Click "ü§ñ C√†o v·ªõi Crawl4AI"** ho·∫∑c "üöÄ C√†o d·ªØ li·ªáu (JS)"
7. **Xem k·∫øt qu·∫£** trong side panel
8. **Click "üíæ Export JSON"** ƒë·ªÉ t·∫£i file

### 2. S·ª≠ D·ª•ng Template

#### 2.1. L∆∞u Template
1. Ch·ªçn c√°c fields nh∆∞ b√¨nh th∆∞·ªùng
2. Click "üìã L∆∞u Template"
3. File JSON s·∫Ω ƒë∆∞·ª£c t·∫£i v·ªÅ

#### 2.2. S·ª≠ D·ª•ng Template v·ªõi Script

```bash
python scrape_with_template.py template.json https://example.com output.json
```

#### 2.3. S·ª≠ D·ª•ng Template v·ªõi Extension
1. Load template t·ª´ file (n·∫øu c√≥ ch·ª©c nƒÉng)
2. Ho·∫∑c s·ª≠ d·ª•ng template ƒë√£ l∆∞u trong storage

### 3. C√†o Images v·ªõi Lazy Loading

1. Ch·ªçn ph·∫ßn t·ª≠ `<img>` ho·∫∑c container ch·ª©a images
2. Ch·ªçn `valueType: "src"`
3. Extension s·∫Ω t·ª± ƒë·ªông:
   - L·∫•y t·ª´ `data-src` n·∫øu c√≥ (lazy loading)
   - Fallback v·ªÅ `src`
   - Filter placeholder v√† SVG

### 4. L·∫•y Container Values

1. Ch·ªçn container ch·ª©a nhi·ªÅu `strong[@itemprop]`
2. Ch·ªçn `valueType: "all"` ho·∫∑c `"container"`
3. K·∫øt qu·∫£ s·∫Ω l√† dictionary v·ªõi key l√† `itemprop` v√† value l√† text

---

## üì° API Documentation

### POST `/`

**Request Body:**
```json
{
  "action": "scrape_with_fields" | "scrape_with_template",
  "url": "https://example.com",
  "fields": [...],  // Cho scrape_with_fields
  "template": {...} // Cho scrape_with_template
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "field1": "value1",
    "field2": ["image1.jpg", "image2.jpg"],
    "field3": {"key1": "value1", "key2": "value2"}
  },
  "url": "https://example.com"
}
```

### Action: `scrape_with_fields`

**Request:**
```json
{
  "action": "scrape_with_fields",
  "url": "https://example.com",
  "fields": [
    {
      "name": "Title",
      "selector": ".title",
      "cssSelector": ".title",
      "xpath": "//div[@class='title']",
      "valueType": "text"
    },
    {
      "name": "Images",
      "selector": ".gallery img",
      "valueType": "src"
    }
  ]
}
```

### Action: `scrape_with_template`

**Request:**
```json
{
  "action": "scrape_with_template",
  "url": "https://example.com",
  "template": {
    "name": "My Template",
    "baseSelector": "body",
    "fields": [...]
  }
}
```

---

## üîÑ X·ª≠ L√Ω D·ªØ Li·ªáu

### 1. Schema Conversion

Extension format ‚Üí Crawl4AI schema:

**Text/HTML:**
```json
{
  "name": "field_name",
  "selector": ".selector",
  "type": "text" | "html"
}
```

**Attribute (src, href, etc.):**
```json
{
  "name": "field_name",
  "selector": ".selector img",
  "type": "list",
  "fields": [{
    "name": "url",
    "type": "attribute",
    "attribute": "src"
  }]
}
```

**Container (all):**
```json
{
  "name": "field_name",
  "selector": "//container//strong[@itemprop]",
  "type": "list",
  "fields": [
    {"name": "value", "type": "text"},
    {"name": "itemprop", "type": "attribute", "attribute": "itemprop"}
  ]
}
```

### 2. Data Post-Processing

**Images:**
- Extract t·ª´ dict: `data_src` > `data-lazy-src` > `src` > `url`
- Filter SVG, placeholder, binary
- Ch·ªâ gi·ªØ URLs

**Container:**
- Convert list of dicts ‚Üí dictionary
- Key: `itemprop`, Value: `value`

**Text/HTML:**
- Trim whitespace
- Handle None/empty values

---

## üêõ Troubleshooting

### 1. Extension kh√¥ng ho·∫°t ƒë·ªông

**Ki·ªÉm tra:**
- Extension ƒë√£ ƒë∆∞·ª£c load ch∆∞a? (`chrome://extensions/`)
- Content script c√≥ ch·∫°y kh√¥ng? (F12 ‚Üí Console)
- C√≥ l·ªói trong background script kh√¥ng?

**Gi·∫£i ph√°p:**
- Reload extension
- Reload trang web
- Ki·ªÉm tra console errors

### 2. API Server kh√¥ng k·∫øt n·ªëi

**Ki·ªÉm tra:**
- Server c√≥ ƒëang ch·∫°y kh√¥ng? (`python extension_api_server.py`)
- Port 8765 c√≥ b·ªã ch·∫∑n kh√¥ng?
- CORS c√≥ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng kh√¥ng?

**Gi·∫£i ph√°p:**
- Restart server
- Ki·ªÉm tra firewall
- Ki·ªÉm tra log terminal

### 3. Selector kh√¥ng ƒë√∫ng

**V·∫•n ƒë·ªÅ:**
- Selector qu√° r·ªông (match nhi·ªÅu elements)
- Selector kh√¥ng match element n√†o
- XPath kh√¥ng ho·∫°t ƒë·ªông v·ªõi Crawl4AI

**Gi·∫£i ph√°p:**
- Ch·ªçn l·∫°i ph·∫ßn t·ª≠
- Toggle gi·ªØa CSS v√† XPath (nh·∫•n `X`)
- Ch·ªânh s·ª≠a selector th·ªß c√¥ng trong code
- S·ª≠ d·ª•ng preview ƒë·ªÉ ki·ªÉm tra

### 4. Images b·ªã null ho·∫∑c placeholder

**Nguy√™n nh√¢n:**
- Lazy loading: URL th·∫≠t ·ªü `data-src`, kh√¥ng ph·∫£i `src`
- Placeholder images ch∆∞a ƒë∆∞·ª£c filter

**Gi·∫£i ph√°p:**
- ƒê√£ ƒë∆∞·ª£c x·ª≠ l√Ω t·ª± ƒë·ªông:
  - ∆Øu ti√™n `data-src` > `src`
  - Filter placeholder images
- N·∫øu v·∫´n l·ªói, ki·ªÉm tra log terminal ƒë·ªÉ debug

### 5. Container extraction kh√¥ng ho·∫°t ƒë·ªông

**Ki·ªÉm tra:**
- `valueType` c√≥ ph·∫£i `"all"` ho·∫∑c `"container"` kh√¥ng?
- Container c√≥ ch·ª©a `strong[@itemprop]` kh√¥ng?
- Selector c√≥ ƒë√∫ng container kh√¥ng?

**Gi·∫£i ph√°p:**
- Ch·ªçn ƒë√∫ng container (div cha)
- ƒê·∫£m b·∫£o c√≥ `itemprop` attributes
- Preview value ƒë·ªÉ ki·ªÉm tra

---

## üìù Ghi Ch√∫ K·ªπ Thu·∫≠t

### 1. Selector Generation

**XPath Strategy:**
1. T√¨m label t·ª´ sibling ho·∫∑c uncle
2. S·ª≠ d·ª•ng `itemprop` n·∫øu c√≥
3. K·∫øt h·ª£p v·ªõi container class
4. Test selector ƒë·ªÉ ƒë·∫£m b·∫£o match ƒë√∫ng 1 element

**CSS Strategy:**
1. ∆Øu ti√™n ID, data attributes
2. S·ª≠ d·ª•ng class names
3. K·∫øt h·ª£p v·ªõi parent selectors

### 2. Lazy Loading Images

**Schema:**
```json
{
  "fields": [
    {"name": "data_src", "type": "attribute", "attribute": "data-src"},
    {"name": "src", "type": "attribute", "attribute": "src"}
  ]
}
```

**Processing:**
```python
v = (v.get('data_src') or 
     v.get('data-lazy-src') or
     v.get('src'))
```

### 3. Debug Logging

API server c√≥ extensive logging:
- Request received
- Schema generation
- Crawl4AI response
- Data processing
- Final formatted data

Xem log trong terminal khi ch·∫°y server.

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- **Crawl4AI**: https://github.com/unclecode/crawl4ai
- **Extension API**: Chrome Extension API documentation
- **XPath**: https://www.w3schools.com/xml/xpath_intro.asp
- **CSS Selectors**: https://www.w3schools.com/cssref/css_selectors.asp

---

## üîÑ Version History

- **v1.0.9**: Current version
  - Lazy loading support
  - Placeholder image filtering
  - Container extraction
  - Improved selector generation
  - Debug logging

---

## üë• ƒê√≥ng G√≥p

ƒê·ªÉ ƒë√≥ng g√≥p ho·∫∑c b√°o l·ªói, vui l√≤ng:
1. Ki·ªÉm tra log terminal
2. M√¥ t·∫£ chi ti·∫øt v·∫•n ƒë·ªÅ
3. Cung c·∫•p URL v√† selector n·∫øu c√≥ th·ªÉ

---

**T√†i li·ªáu n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t l·∫ßn cu·ªëi: 2025-12-11**




@'
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        ctx = await p.chromium.launch_persistent_context(
            "playwright_profile",
            headless=False,
            viewport={"width": 1366, "height": 768},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            )
        )
        page = await ctx.new_page()
        await page.goto("https://batdongsan.com.vn/nha-dat-ban", wait_until="domcontentloaded", timeout=60000)
        print("B·∫•m human/captcha n·∫øu c√≥, r·ªìi ƒë·ª£i (t·ªëi ƒëa 120s). C·ª≠a s·ªï s·∫Ω t·ª± ƒë√≥ng.")
        await page.wait_for_timeout(120000)
        await ctx.close()

asyncio.run(main())
'@ | python -