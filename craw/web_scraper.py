"""
Tool c√†o d·ªØ li·ªáu t·ª´ trang web s·ª≠ d·ª•ng Crawl4AI
H·ªó tr·ª£ nhi·ªÅu t√≠nh nƒÉng: c√†o ƒë∆°n gi·∫£n, extract theo schema, extract v·ªõi LLM
"""

import asyncio
import json
import os
import sys
from pathlib import Path
from typing import Optional, Dict, List, Any
from datetime import datetime

# Force UTF-8 to avoid Windows console encoding errors (rich logger)
os.environ.setdefault("PYTHONIOENCODING", "utf-8")
for _stream in (sys.stdout, sys.stderr):
    if hasattr(_stream, "reconfigure"):
        try:
            _stream.reconfigure(encoding="utf-8", errors="ignore")
        except Exception:
            pass

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
from crawl4ai import JsonCssExtractionStrategy, LLMExtractionStrategy, RegexExtractionStrategy
from crawl4ai import LLMConfig
from crawl4ai import PruningContentFilter, BM25ContentFilter


class WebScraper:
    """Tool c√†o d·ªØ li·ªáu t·ª´ trang web"""
    
    # Counter ƒë·ªÉ t·∫°o unique ID cho m·ªói instance
    _instance_counter = 0
    
    # S·ª≠a trong file web_scraper.py
    def __init__(
        self,
        headless: bool = True,
        verbose: bool = False,
        keep_open: bool = False,
        user_data_dir: str = None,
        use_managed_browser: bool = True,
    ):
        """
        Kh·ªüi t·∫°o WebScraper
        Args:
            user_data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u Profile (Cookie, Cache...)
        """
        # T·∫°o unique instance ID ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã share browser
        WebScraper._instance_counter += 1
        self._instance_id = WebScraper._instance_counter
        import time
        self._unique_id = f"{self._instance_id}_{int(time.time() * 1000)}"
        print(f"[WebScraper] Creating instance #{self._instance_id} with unique_id={self._unique_id}")
        
        self.keep_open = keep_open

        stealth_args = [
            "--disable-blink-features=AutomationControlled", # Quan tr·ªçng nh·∫•t: T·∫Øt d·∫•u hi·ªáu Robot
            "--no-first-run",
            "--disable-infobars",
            "--disable-gpu",
            "--disable-dev-shm-usage",
            "--no-sandbox",
            "--ignore-certificate-errors",
        ]
        extra_args = list(stealth_args)
        if not headless:
            extra_args.extend([
                "--start-maximized",
                "--window-position=0,0",
                "--disable-features=CalculateNativeWinOcclusion",
                "--disable-backgrounding-occluded-windows",
                "--disable-session-crashed-bubble",
            ])
        
        # Th√™m unique window name ƒë·ªÉ tr√°nh browser b·ªã share
        extra_args.append(f"--class=crawl4ai-instance-{self._unique_id}")
        
        ua_xin = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
        
        # X√°c ƒë·ªãnh c√≥ d√πng persistent context hay kh√¥ng
        # Ch·ªâ b·∫≠t persistent context khi c√≥ user_data_dir
        use_persistent = bool(user_data_dir)
        
        # T√≠nh debugging port unique d·ª±a tr√™n instance ID ƒë·ªÉ tr√°nh conflict
        # Base port 9222, m·ªói instance +1
        unique_debug_port = 9222 + (self._instance_id % 100)
        
        self.browser_config = BrowserConfig(
            headless=headless,
            verbose=verbose,
            browser_mode="dedicated",  # D√πng dedicated browser, kh√¥ng chia s·∫ª
            debugging_port=unique_debug_port,  # Unique port cho m·ªói instance
            viewport_width=1920,
            viewport_height=1040,
            user_data_dir=user_data_dir,
            use_managed_browser=use_managed_browser,
            use_persistent_context=use_persistent,  # Ch·ªâ b·∫≠t khi c√≥ user_data_dir
            extra_args=extra_args,
            user_agent=ua_xin
        )
        print(f"[WebScraper] Instance #{self._instance_id}: debug_port={unique_debug_port}, user_data_dir={user_data_dir}")
        self.crawler = None
    
    async def __aenter__(self):
        """Context manager entry"""
        self.crawler = AsyncWebCrawler(config=self.browser_config)
        await self.crawler.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        # Gi·ªØ browser m·ªü n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu (d√πng cho quan s√°t th·ªß c√¥ng)
        if self.crawler and not self.keep_open:
            await self.crawler.__aexit__(exc_type, exc_val, exc_tb)
    
    async def close(self):
        """ƒê√≥ng browser th·ªß c√¥ng khi keep_open=True"""
        if self.crawler:
            await self.crawler.__aexit__(None, None, None)

    async def get_active_page(self):
        """
        L·∫•y page ƒëang active t·ª´ Crawl4AI's browser context.
        D√πng cho vi·ªác hi·ªÉn th·ªã v√† c√†o tr·ª±c ti·∫øp.
        QUAN TR·ªåNG: Kh√¥ng t·∫°o page m·ªõi khi context ƒë√£ ƒë√≥ng - tr·∫£ v·ªÅ None ƒë·ªÉ caller x·ª≠ l√Ω.
        """
        try:
            ctx = getattr(self.crawler, "crawler_strategy", None)
            if not ctx:
                print(f"[WebScraper #{self._instance_id}] get_active_page: No crawler_strategy")
                return None
            bm = getattr(ctx, "browser_manager", None)
            if not bm:
                print(f"[WebScraper #{self._instance_id}] get_active_page: No browser_manager")
                return None
            
            # Debug: Log browser manager v√† context ƒë·ªÉ x√°c ƒë·ªãnh c√≥ b·ªã share kh√¥ng
            bm_id = id(bm)
            crawler_id = id(self.crawler)
            user_data = getattr(self.browser_config, 'user_data_dir', 'N/A')
            print(f"[WebScraper #{self._instance_id}] get_active_page: crawler_id={crawler_id}, browser_manager_id={bm_id}, user_data_dir={user_data}")
            
            context = getattr(bm, "default_context", None)
            if not context:
                print(f"[WebScraper #{self._instance_id}] get_active_page: No default_context")
                return None
            
            ctx_id = id(context)
            print(f"[WebScraper #{self._instance_id}] get_active_page: context_id={ctx_id}")
            
            # Ki·ªÉm tra context c√≤n s·ªëng kh√¥ng tr∆∞·ªõc khi truy c·∫≠p pages
            try:
                # Th·ª≠ truy c·∫≠p thu·ªôc t√≠nh c·ªßa context ƒë·ªÉ ki·ªÉm tra c√≤n s·ªëng
                if hasattr(context, '_closed') and context._closed:
                    print("[WebScraper] Context is closed, returning None")
                    return None
            except Exception:
                pass
            
            pages = []
            try:
                pages = list(context.pages) if hasattr(context, 'pages') else []
            except Exception as e:
                # Context c√≥ th·ªÉ ƒë√£ b·ªã ƒë√≥ng
                print(f"[WebScraper] Cannot get pages from context: {e}")
                return None
            
            page = None
            if pages:
                # Tr·∫£ v·ªÅ page cu·ªëi c√πng (th∆∞·ªùng l√† page ƒëang active)
                # Ki·ªÉm tra page c√≤n s·ªëng kh√¥ng
                for p in reversed(pages):
                    try:
                        # Ki·ªÉm tra page c√≤n s·ªëng b·∫±ng c√°ch truy c·∫≠p url
                        _ = p.url
                        page = p
                        break
                    except Exception:
                        continue
                        
            # KH√îNG t·∫°o page m·ªõi n·∫øu kh√¥ng c√≥ page n√†o - ƒë·ªÉ caller quy·∫øt ƒë·ªãnh
            # Vi·ªác t·∫°o page m·ªõi khi context ƒë√≥ng s·∫Ω g√¢y l·ªói "Target page, context or browser has been closed"
            if not page and pages:
                # C√≥ pages nh∆∞ng t·∫•t c·∫£ ƒë·ªÅu ƒë√£ ƒë√≥ng
                print("[WebScraper] All pages are closed")
                return None
            elif not page and not pages:
                # Kh√¥ng c√≥ page n√†o, th·ª≠ t·∫°o m·ªõi N·∫æU context c√≤n s·ªëng
                try:
                    page = await context.new_page()
                except Exception as e:
                    print(f"[WebScraper] Cannot create new page: {e}")
                    return None
            
            # Set viewport to match window size (full screen)
            if page:
                try:
                    # L·∫•y k√≠ch th∆∞·ªõc m√†n h√¨nh th·ª±c t·∫ø v√† set viewport
                    await page.set_viewport_size({"width": 1920, "height": 1040})
                except Exception as e:
                    print(f"[WebScraper] set_viewport_size error: {e}")
            
            return page
        except Exception as e:
            print(f"[WebScraper] get_active_page error: {e}")
            return None

    async def navigate_and_get_html(self, url: str, wait_networkidle: bool = True, timeout: int = 30000) -> dict:
        """
        Navigate page hi·ªÉn th·ªã ƒë·∫øn URL v√† l·∫•y HTML.
        D√πng display_page n·∫øu c√≥, kh√¥ng th√¨ l·∫•y active page t·ª´ Crawl4AI.
        """
        page = getattr(self, 'display_page', None)
        if not page:
            page = await self.get_active_page()
        if not page:
            return {"success": False, "error": "No page available", "html": ""}
        
        try:
            print(f"[WebScraper] Navigating to: {url[:100]}")
            await page.goto(url, wait_until="domcontentloaded", timeout=timeout)
            if wait_networkidle:
                try:
                    await page.wait_for_load_state("networkidle", timeout=15000)
                except Exception:
                    pass
            try:
                await page.bring_to_front()
            except Exception:
                pass
            html = await page.content()
            current_url = page.url or url
            print(f"[WebScraper] Got HTML, length={len(html)}, url={current_url[:80]}")
            self.display_page = page
            return {"success": True, "html": html, "url": current_url, "page": page}
        except Exception as e:
            print(f"[WebScraper] navigate_and_get_html error: {e}")
            return {"success": False, "error": str(e), "html": ""}
    
    async def scrape_simple(self, url: str, bypass_cache: bool = False) -> Dict[str, Any]:
        """
        C√†o d·ªØ li·ªáu ƒë∆°n gi·∫£n - l·∫•y markdown v√† HTML
        
        Args:
            url: URL c·∫ßn c√†o
            bypass_cache: B·ªè qua cache
            
        Returns:
            Dict ch·ª©a markdown, HTML v√† metadata
        """
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED
        )
        
        result = await self.crawler.arun(url=url, config=config)
        
        return {
            "success": result.success,
            "url": url,
            "markdown": result.markdown.raw_markdown if result.success else None,
            "html": result.html if result.success else None,  # <--- S·ª≠a ·ªü ƒë√¢y: L·∫•y HTML g·ªëc
            "title": result.metadata.get("title", "") if result.success else "",
            "description": result.metadata.get("description", "") if result.success else "",
            "error": result.error_message if not result.success else None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def scrape_with_js(self, url: str, js_code: List[str], bypass_cache: bool = False) -> Dict[str, Any]:
        """
        C√†o d·ªØ li·ªáu v·ªõi JavaScript code ƒë·ªÉ thao t√°c trang
        
        Args:
            url: URL c·∫ßn c√†o
            js_code: List c√°c d√≤ng JavaScript code
            bypass_cache: B·ªè qua cache
            
        Returns:
            Dict ch·ª©a markdown, HTML v√† metadata
        """
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED,
            js_code=js_code
        )
        
        result = await self.crawler.arun(url=url, config=config)
        
        return {
            "success": result.success,
            "url": url,
            "markdown": result.markdown.raw_markdown if result.success else None,
            "html": result.html if result.success else None,  # <--- S·ª≠a ·ªü ƒë√¢y: L·∫•y HTML g·ªëc
            "title": result.metadata.get("title", "") if result.success else "",
            "description": result.metadata.get("description", "") if result.success else "",
            "error": result.error_message if not result.success else None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def scrape_with_schema(self, url: str, schema: Dict, bypass_cache: bool = False) -> Dict[str, Any]:
        """
        C√†o d·ªØ li·ªáu v·ªõi schema CSS selector
        
        Args:
            url: URL c·∫ßn c√†o
            schema: Schema ƒë·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu c·∫ßn extract
            bypass_cache: B·ªè qua cache
            
        Example schema:
            {
                "name": "Articles",
                "baseSelector": "article.post",
                "fields": [
                    {"name": "title", "selector": "h2", "type": "text"},
                    {"name": "url", "selector": "a", "type": "attribute", "attribute": "href"}
                ]
            }
        """
        # ƒê·∫£m b·∫£o schema c√≥ baseSelector (Crawl4AI y√™u c·∫ßu b·∫Øt bu·ªôc)
        if 'baseSelector' not in schema or not schema.get('baseSelector'):
            schema['baseSelector'] = 'body'
        
        # ƒê·∫£m b·∫£o schema c√≥ name
        if 'name' not in schema:
            schema['name'] = 'ExtractedData'
        
        # ƒê·∫£m b·∫£o schema c√≥ fields
        if 'fields' not in schema:
            schema['fields'] = []
        
        # Debug: Log schema tr∆∞·ªõc khi t·∫°o strategy
        print(f"\n[WebScraper] Schema tr∆∞·ªõc khi t·∫°o JsonCssExtractionStrategy:")
        print(f"  name: {schema.get('name')}")
        print(f"  baseSelector: {schema.get('baseSelector')}")
        for i, field in enumerate(schema.get('fields', [])):
            field_selector = field.get('selector', 'N/A')
            is_xpath = field_selector.startswith('//') if field_selector != 'N/A' else False
            print(f"  Field {i+1}: name='{field.get('name')}', selector='{field_selector[:80]}...', is_xpath={is_xpath}")
        
        extraction_strategy = JsonCssExtractionStrategy(schema)
        
        config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED,
            word_count_threshold=10
        )
        
        print(f"[WebScraper] ƒêang scrape URL: {url[:80]}...")
        result = await self.crawler.arun(url=url, config=config)
        
        # Debug: Log k·∫øt qu·∫£
        print(f"[WebScraper] K·∫øt qu·∫£ scrape:")
        print(f"  success: {result.success}")
        if result.success:
            print(f"  extracted_content type: {type(result.extracted_content)}")
            if result.extracted_content:
                print(f"  extracted_content preview: {str(result.extracted_content)[:200]}...")
        else:
            print(f"  error_message: {result.error_message}")
        
        extracted_data = None
        if result.success and result.extracted_content:
            try:
                extracted_data = json.loads(result.extracted_content)
            except json.JSONDecodeError:
                extracted_data = result.extracted_content
        
        return {
            "success": result.success,
            "url": url,
            "extracted_data": extracted_data,
            "markdown": result.markdown.raw_markdown if result.success else None,
            "error": result.error_message if not result.success else None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def scrape_with_regex(self, url: str, patterns: Optional[Dict[str, str]] = None, 
                                builtin_patterns: Optional[int] = None, bypass_cache: bool = False) -> Dict[str, Any]:
        """
        C√†o d·ªØ li·ªáu s·ª≠ d·ª•ng regex patterns
        
        Args:
            url: URL c·∫ßn c√†o
            patterns: Dict custom patterns {label: regex}
            builtin_patterns: Bit flags cho built-in patterns (Email, Phone, etc.)
            bypass_cache: B·ªè qua cache
        """
        extraction_strategy = RegexExtractionStrategy(
            pattern=builtin_patterns or RegexExtractionStrategy.All,
            custom=patterns
        )
        
        config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED
        )
        
        result = await self.crawler.arun(url=url, config=config)
        
        extracted_data = None
        if result.success and result.extracted_content:
            try:
                extracted_data = json.loads(result.extracted_content)
            except json.JSONDecodeError:
                extracted_data = result.extracted_content
        
        return {
            "success": result.success,
            "url": url,
            "extracted_data": extracted_data,
            "markdown": result.markdown.raw_markdown if result.success else None,
            "error": result.error_message if not result.success else None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def scrape_with_llm(self, url: str, instruction: str, 
                             llm_config: Optional[LLMConfig] = None, 
                             bypass_cache: bool = False) -> Dict[str, Any]:
        """
        C√†o d·ªØ li·ªáu s·ª≠ d·ª•ng LLM ƒë·ªÉ extract th√¥ng minh
        
        Args:
            url: URL c·∫ßn c√†o
            instruction: H∆∞·ªõng d·∫´n cho LLM v·ªÅ d·ªØ li·ªáu c·∫ßn extract
            llm_config: Config cho LLM (n·∫øu None s·∫Ω d√πng OpenAI t·ª´ env)
            bypass_cache: B·ªè qua cache
        """
        # N·∫øu kh√¥ng c√≥ llm_config, th·ª≠ l·∫•y t·ª´ env
        if llm_config is None:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                llm_config = LLMConfig(
                    provider="openai/gpt-4o-mini",
                    api_token=api_key
                )
            else:
                return {
                    "success": False,
                    "url": url,
                    "error": "C·∫ßn cung c·∫•p LLM config ho·∫∑c set OPENAI_API_KEY trong env",
                    "timestamp": datetime.now().isoformat()
                }
        
        extraction_strategy = LLMExtractionStrategy(
            llm_config=llm_config,
            extraction_type="schema",
            instruction=instruction
        )
        
        config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED,
            word_count_threshold=50
        )
        
        result = await self.crawler.arun(url=url, config=config)
        
        extracted_data = None
        if result.success and result.extracted_content:
            try:
                extracted_data = json.loads(result.extracted_content)
            except json.JSONDecodeError:
                extracted_data = result.extracted_content
        
        return {
            "success": result.success,
            "url": url,
            "extracted_data": extracted_data,
            "markdown": result.markdown.raw_markdown if result.success else None,
            "error": result.error_message if not result.success else None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def scrape_multiple(self, urls: List[str], bypass_cache: bool = False) -> List[Dict[str, Any]]:
        """
        C√†o nhi·ªÅu URLs c√πng l√∫c
        
        Args:
            urls: List c√°c URLs c·∫ßn c√†o
            bypass_cache: B·ªè qua cache
            
        Returns:
            List c√°c k·∫øt qu·∫£
        """
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if bypass_cache else CacheMode.ENABLED
        )
        
        results = await self.crawler.arun_many(urls=urls, config=config)
        
        output = []
        for result in results:
            output.append({
                "success": result.success,
                "url": result.url if hasattr(result, 'url') else "",
                "markdown": result.markdown.raw_markdown if result.success else None,
                "html": result.html if result.success else None,  # <--- S·ª≠a ·ªü ƒë√¢y: L·∫•y HTML g·ªëc
                "error": result.error_message if not result.success else None,
                "timestamp": datetime.now().isoformat()
            })
        
        return output
    
    def save_to_file(self, data: Dict[str, Any], filename: Optional[str] = None, 
                    output_dir: str = "output") -> str:
        """
        L∆∞u k·∫øt qu·∫£ v√†o file JSON
        
        Args:
            data: D·ªØ li·ªáu c·∫ßn l∆∞u
            filename: T√™n file (n·∫øu None s·∫Ω t·ª± ƒë·ªông generate)
            output_dir: Th∆∞ m·ª•c output
            
        Returns:
            ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        Path(output_dir).mkdir(exist_ok=True)
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"scrape_{timestamp}.json"
        
        filepath = Path(output_dir) / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)


# H√†m helper ƒë·ªÉ s·ª≠ d·ª•ng d·ªÖ d√†ng
async def quick_scrape(url: str, output_file: Optional[str] = None) -> Dict[str, Any]:
    """
    H√†m helper ƒë·ªÉ c√†o nhanh m·ªôt URL
    
    Args:
        url: URL c·∫ßn c√†o
        output_file: File ƒë·ªÉ l∆∞u k·∫øt qu·∫£ (optional)
    
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£
    """
    async with WebScraper() as scraper:
        result = await scraper.scrape_simple(url)
        
        if output_file:
            scraper.save_to_file(result, output_file)
            print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_file}")
        
        return result


if __name__ == "__main__":
    # Example usage
    async def main():
        async with WebScraper(headless=True, verbose=True) as scraper:
            # Test v·ªõi m·ªôt URL ƒë∆°n gi·∫£n
            print(" ƒêang c√†o d·ªØ li·ªáu t·ª´ example.com...")
            result = await scraper.scrape_simple("https://example.com")
            
            if result["success"]:
                print(f"‚úÖ Th√†nh c√¥ng!")
                print(f"üìÑ Markdown length: {len(result['markdown'])} chars")
                print(f"üìù Title: {result['title']}")
                print(f"\n--- Markdown preview ---\n{result['markdown'][:500]}")
                
                # L∆∞u k·∫øt qu·∫£
                filepath = scraper.save_to_file(result)
                print(f"\nüíæ ƒê√£ l∆∞u v√†o: {filepath}")
            else:
                print(f"‚ùå L·ªói: {result['error']}")
    
    asyncio.run(main())
