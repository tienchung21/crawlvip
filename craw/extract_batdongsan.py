"""
Tool extract th√¥ng tin b·∫•t ƒë·ªông s·∫£n t·ª´ batdongsan.com.vn
K·∫øt h·ª£p CSS Schema + HTML parsing + AI (t√πy ch·ªçn)

C√°ch s·ª≠ d·ª•ng:
  python extract_batdongsan.py <URL> [output_file] [--no-ai]
  
V√≠ d·ª•:


  python extract_batdongsan.py ""
"""

import asyncio
import os
import json
import re
import sys
from datetime import datetime
from bs4 import BeautifulSoup
from web_scraper import WebScraper
from crawl4ai import LLMConfig, JsonCssExtractionStrategy

# Set Groq API key
GROQ_API_KEY = "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"
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# Decrypt phone config - import t·ª´ file ri√™ng
try:
    from decrypt_config import DECRYPT_ENABLED, COOKIES, USER_AGENT
    DECRYPT_PHONE_ENABLED = DECRYPT_ENABLED
    COOKIES_FOR_DECRYPT = COOKIES
    USER_AGENT_FOR_DECRYPT = USER_AGENT
except ImportError:
    DECRYPT_PHONE_ENABLED = False
    COOKIES_FOR_DECRYPT = ""
    USER_AGENT_FOR_DECRYPT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"


def decrypt_phone_number(encrypted_phone: str) -> str:
    """
    Decrypt phone number using API
    Requires: pip install curl_cffi
    """
    if not DECRYPT_PHONE_ENABLED or not COOKIES_FOR_DECRYPT or not encrypted_phone:
        return None
    
    try:
        from curl_cffi import requests
        
        # Parse cookies
        cookies = {}
        for cookie in COOKIES_FOR_DECRYPT.strip().split('; '):
            if '=' in cookie:
                key, value = cookie.split('=', 1)
                cookies[key.strip()] = value.strip()
        
        headers = {
            'Accept': '*/*',
            'Accept-Language': 'vi,en-US;q=0.9,en;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Origin': 'https://batdongsan.com.vn',
            'Referer': 'https://batdongsan.com.vn/',
            'User-Agent': USER_AGENT_FOR_DECRYPT.strip(),
            'X-Requested-With': 'XMLHttpRequest',
        }
        
        data = {
            'PhoneNumber': encrypted_phone,
            'createLead[mobile]': '4883611',
            'createLead[sellerId]': '4491058',
            'createLead[productId]': '44503982',
            'createLead[leadSourcePage]': 'BDS_LISTING_DETAILS_PAGE',
            'createLead[leadSourceAction]': 'PHONE_REVEAL',
            'createLead[fromLeadType]': 'AGENT_LISTING'
        }
        
        response = requests.post(
            "https://batdongsan.com.vn/Product/ProductDetail/DecryptPhone",
            headers=headers,
            cookies=cookies,
            data=data,
            timeout=10,
            impersonate="chrome120"
        )
        
        if response.status_code == 200:
            phone = response.text.strip()
            if phone and len(phone) >= 9:
                return phone
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Decrypt failed: {e}")
        return None


def clean_text(text: str) -> str:
    """L√†m s·∫°ch text, lo·∫°i b·ªè markdown, link, etc."""
    if not text:
        return ""
    
    # Lo·∫°i b·ªè markdown links
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    # Lo·∫°i b·ªè markdown images
    text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', text)
    # Lo·∫°i b·ªè HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Lo·∫°i b·ªè multiple spaces
    text = re.sub(r'\s+', ' ', text)
    # Trim
    text = text.strip()
    
    return text


def extract_from_html(html: str, raw_html: str = None) -> dict:
    """
    Extract th√¥ng tin t·ª´ HTML b·∫±ng BeautifulSoup
    
    Args:
        html: Cleaned HTML ƒë·ªÉ parse
        raw_html: Raw HTML (c√≥ iframe) ƒë·ªÉ extract t·ªça ƒë·ªô
    """
    soup = BeautifulSoup(html, 'html.parser')
    
    # N·∫øu kh√¥ng c√≥ raw_html, d√πng html
    if raw_html is None:
        raw_html = html
    data = {}
    
    # Title - t√¨m trong h1 (sau breadcrumb)
    h1 = soup.find('h1')
    if h1:
        title_text = clean_text(h1.get_text())
        # Lo·∫°i b·ªè ph·∫ßn logo n·∫øu c√≥
        if 'N·ªÅn t·∫£ng b·∫•t ƒë·ªông s·∫£n' not in title_text and len(title_text) > 5:
            data['title'] = title_text
    
    # Fallback: t√¨m h1 v·ªõi class c·ª• th·ªÉ
    if not data.get('title'):
        h1_with_class = soup.select_one('h1.re__pr-title, h1[class*="title"]')
        if h1_with_class:
            title_text = clean_text(h1_with_class.get_text())
            if len(title_text) > 10:
                data['title'] = title_text
    
    # ƒê·ªãa ch·ªâ - t√¨m span ngay sau h1
    if h1:
        next_span = h1.find_next_sibling('span')
        if next_span:
            address = clean_text(next_span.get_text())
            if address and len(address) > 5:
                data['dia_chi'] = address
    
    # Gi√° - t√¨m span c√≥ text "Kho·∫£ng gi√°" r·ªìi l·∫•y span k·∫ø ti·∫øp
    price_label = soup.find('span', string=re.compile(r'Kho·∫£ng gi√°', re.I))
    if price_label:
        price_value = price_label.find_next_sibling('span')
        if price_value:
            data['khoang_gia'] = clean_text(price_value.get_text())
    
    # Fallback: t√¨m b·∫•t k·ª≥ element n√†o c√≥ text "t·ª∑" ho·∫∑c "tri·ªáu"
    if not data.get('khoang_gia'):
        price_elems = soup.find_all(string=re.compile(r'\d+[\.,]?\d*\s*(t·ª∑|tri·ªáu|tr|ngh√¨n)', re.I))
        for elem in price_elems:
            price_text = clean_text(elem)
            # Ki·ªÉm tra c√≥ s·ªë v√† ƒë∆°n v·ªã ti·ªÅn t·ªá
            if re.search(r'\d+[\.,]?\d*\s*(t·ª∑|tri·ªáu)', price_text, re.I):
                # Kh√¥ng l·∫•y n·∫øu n·∫±m trong m√¥ t·∫£ d√†i
                if len(price_text) < 100:
                    data['khoang_gia'] = price_text
                    break
    
    # Di·ªán t√≠ch - t√¨m span c√≥ text "Di·ªán t√≠ch" r·ªìi l·∫•y span k·∫ø ti·∫øp
    # T√¨m t·∫•t c·∫£ c√°c span "Di·ªán t√≠ch" v√† l·∫•y c√°i ƒë·∫ßu ti√™n h·ª£p l·ªá
    area_labels = soup.find_all('span', string=re.compile(r'Di·ªán t√≠ch', re.I))
    for area_label in area_labels:
        # Ki·ªÉm tra xem c√≥ n·∫±m trong ph·∫ßn m√¥i gi·ªõi kh√¥ng (c√≥ link guru trong parent)
        parent_div = area_label.find_parent('div')
        if parent_div:
            # N·∫øu parent c√≥ link guru th√¨ b·ªè qua (ƒë√¢y l√† ph·∫ßn m√¥i gi·ªõi)
            if parent_div.find('a', href=re.compile(r'guru\.batdongsan\.com\.vn', re.I)):
                continue
        
        area_value = area_label.find_next_sibling('span')
        if area_value:
            area_text = clean_text(area_value.get_text())
            # Ch·ªâ l·∫•y n·∫øu c√≥ m¬≤ ho·∫∑c m2 v√† c√≥ s·ªë trong ƒë√≥
            if ('m¬≤' in area_text or 'm2' in area_text.lower()):
                # Ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng s·ªë (kh√¥ng ph·∫£i t√™n)
                if re.match(r'^\d+', area_text):
                    # Kh√¥ng ch·ª©a "Xem th√™m", "tin kh√°c", "Bƒës" (t√™n c√¥ng ty)
                    if not any(word in area_text for word in ['Xem th√™m', 'tin kh√°c', 'Bƒës', 'BDS']):
                        # Ph·∫£i c√≥ ƒë·ªô d√†i h·ª£p l√Ω (kh√¥ng qu√° d√†i)
                        if len(area_text) < 20:
                            data['dien_tich'] = area_text
                            break
    
    # Fallback: t√¨m trong ph·∫ßn "ƒê·∫∑c ƒëi·ªÉm b·∫•t ƒë·ªông s·∫£n" (n·∫øu c√≥)
    if not data.get('dien_tich'):
        # T√¨m span "Di·ªán t√≠ch" trong ph·∫ßn ƒë·∫∑c ƒëi·ªÉm
        dac_diem_section = soup.find('div', string=re.compile(r'ƒê·∫∑c ƒëi·ªÉm', re.I))
        if dac_diem_section:
            parent = dac_diem_section.find_parent()
            if parent:
                area_spans = parent.find_all('span', string=re.compile(r'Di·ªán t√≠ch', re.I))
                for span in area_spans:
                    next_span = span.find_next_sibling('span')
                    if next_span:
                        area_val = clean_text(next_span.get_text())
                        if re.match(r'^\d+.*m[¬≤2]', area_val) and len(area_val) < 20:
                            data['dien_tich'] = area_val
                            break
    

    
    # ƒê·∫∑c ƒëi·ªÉm b·∫•t ƒë·ªông s·∫£n (kho·∫£ng gi√°, di·ªán t√≠ch, h∆∞·ªõng nh√†, m·∫∑t ti·ªÅn, ƒë∆∞·ªùng v√†o, ph√°p l√Ω)
    dac_diem = {}
    
    # Kho·∫£ng gi√° (l·∫•y t·ª´ ƒë√£ extract ·ªü tr√™n)
    if data.get('khoang_gia'):
        dac_diem['khoang_gia'] = data['khoang_gia']
    
    # Di·ªán t√≠ch (l·∫•y t·ª´ ƒë√£ extract ·ªü tr√™n)
    if data.get('dien_tich'):
        dac_diem['dien_tich'] = data['dien_tich']
    
    # H∆∞·ªõng nh√†
    huong_label = soup.find('span', string=re.compile(r'H∆∞·ªõng nh√†', re.I))
    if huong_label:
        huong_value = huong_label.find_next_sibling('span')
        if huong_value:
            huong_text = clean_text(huong_value.get_text())
            if huong_text and len(huong_text) < 50:
                dac_diem['huong_nha'] = huong_text
    
    # M·∫∑t ti·ªÅn
    mat_tien_label = soup.find('span', string=re.compile(r'M·∫∑t ti·ªÅn', re.I))
    if mat_tien_label:
        mat_tien_value = mat_tien_label.find_next_sibling('span')
        if mat_tien_value:
            mat_tien_text = clean_text(mat_tien_value.get_text())
            if mat_tien_text and len(mat_tien_text) < 50 and re.search(r'\d', mat_tien_text):
                dac_diem['mat_tien'] = mat_tien_text
    
    # ƒê∆∞·ªùng v√†o
    duong_label = soup.find('span', string=re.compile(r'ƒê∆∞·ªùng v√†o', re.I))
    if duong_label:
        duong_value = duong_label.find_next_sibling('span')
        if duong_value:
            duong_text = clean_text(duong_value.get_text())
            if duong_text and len(duong_text) < 50:
                dac_diem['duong_vao'] = duong_text
    
    # Ph√°p l√Ω
    phap_ly_label = soup.find('span', string=re.compile(r'Ph√°p l√Ω', re.I))
    if phap_ly_label:
        phap_ly_value = phap_ly_label.find_next_sibling('span')
        if phap_ly_value:
            phap_ly_text = clean_text(phap_ly_value.get_text())
            if phap_ly_text and len(phap_ly_text) < 100:
                dac_diem['phap_ly'] = phap_ly_text
    
    if dac_diem:
        data['dac_diem'] = dac_diem
    
    # M√¥ t·∫£ - t√¨m div sau span "Th√¥ng tin m√¥ t·∫£"
    desc_label = soup.find('span', string=re.compile(r'Th√¥ng tin m√¥ t·∫£', re.I))
    if desc_label:
        desc_div = desc_label.find_next('div')
        if desc_div:
            desc = clean_text(desc_div.get_text())
            if desc and len(desc) > 20:
                data['mo_ta'] = desc
    
    # T√™n d·ª± √°n - t√¨m trong ph·∫ßn "Th√¥ng tin d·ª± √°n"
    du_an_label = soup.find('div', string=re.compile(r'Th√¥ng tin d·ª± √°n', re.I))
    if du_an_label:
        # T√¨m div ch·ª©a t√™n d·ª± √°n
        parent = du_an_label.find_parent()
        if parent:
            # T√¨m t·∫•t c·∫£ div v√† span con
            name_elems = parent.find_all(['div', 'span', 'a'], recursive=True)
            for elem in name_elems:
                elem_text = clean_text(elem.get_text())
                # T√¨m text ng·∫Øn (10-100 k√Ω t·ª±) c√≥ ch·ªØ in hoa
                if 10 < len(elem_text) < 100:
                    # Lo·∫°i b·ªè c√°c text kh√¥ng ph·∫£i t√™n d·ª± √°n
                    if 'Xem' not in elem_text and 'tin ƒëƒÉng' not in elem_text.lower() and 'ƒêang c·∫≠p nh·∫≠t' not in elem_text:
                        # Ki·ªÉm tra xem c√≥ ph·∫£i t√™n d·ª± √°n kh√¥ng (kh√¥ng ph·∫£i link, kh√¥ng ph·∫£i s·ªë)
                        if not elem_text.startswith('http') and not re.match(r'^\d+$', elem_text) and not elem_text.startswith('¬∑'):
                            # Ph·∫£i c√≥ √≠t nh·∫•t 1 ch·ªØ in hoa ho·∫∑c c√≥ d·∫•u g·∫°ch ngang
                            if re.search(r'[A-Zƒê]', elem_text) or '-' in elem_text:
                                # T√™n d·ª± √°n th∆∞·ªùng ng·∫Øn (2-10 t·ª´)
                                word_count = len(elem_text.split())
                                if 2 <= word_count <= 10:
                                    data['du_an'] = {'ten': elem_text}
                                    break
            
            # T√¨m link
            du_an_link = parent.find('a', href=re.compile(r'du-an|the-paris|vinhomes', re.I))
            if du_an_link:
                link = du_an_link.get('href', '')
                if link:
                    if not link.startswith('http'):
                        link = 'https://batdongsan.com.vn' + link
                    if not data.get('du_an'):
                        data['du_an'] = {}
                    data['du_an']['link'] = link
    
    # Ng√†y ƒëƒÉng - t√¨m span "Ng√†y ƒëƒÉng" r·ªìi l·∫•y span k·∫ø ti·∫øp
    ngay_dang_label = soup.find('span', string=re.compile(r'Ng√†y ƒëƒÉng', re.I))
    if ngay_dang_label:
        ngay_dang_value = ngay_dang_label.find_next_sibling('span')
        if ngay_dang_value:
            data['ngay_dang'] = clean_text(ngay_dang_value.get_text())
    
    # Ng√†y h·∫øt h·∫°n
    ngay_het_han_label = soup.find('span', string=re.compile(r'Ng√†y h·∫øt h·∫°n', re.I))
    if ngay_het_han_label:
        ngay_het_han_value = ngay_het_han_label.find_next_sibling('span')
        if ngay_het_han_value:
            data['ngay_het_han'] = clean_text(ngay_het_han_value.get_text())
    
    # Lo·∫°i tin - t√¨m span "Lo·∫°i tin" r·ªìi l·∫•y span k·∫ø ti·∫øp
    loai_tin_label = soup.find('span', string=re.compile(r'Lo·∫°i tin', re.I))
    if loai_tin_label:
        loai_tin_value = loai_tin_label.find_next_sibling('span')
        if loai_tin_value:
            data['loai_tin'] = clean_text(loai_tin_value.get_text())
    
    # M√£ tin - t√¨m span "M√£ tin" r·ªìi l·∫•y span k·∫ø ti·∫øp
    ma_tin_label = soup.find('span', string=re.compile(r'M√£ tin', re.I))
    if ma_tin_label:
        ma_tin_value = ma_tin_label.find_next_sibling('span')
        if ma_tin_value:
            data['ma_tin'] = clean_text(ma_tin_value.get_text())
    
    # M√¥i gi·ªõi - t√¨m trong th·∫ª a c√≥ href ch·ª©a "guru.batdongsan.com.vn"
    moi_gioi = {}
    
    # T√¨m t·∫•t c·∫£ th·∫ª a c√≥ href ch·ª©a "guru.batdongsan.com.vn"
    guru_links = soup.find_all('a', href=re.compile(r'guru\.batdongsan\.com\.vn', re.I))
    for link in guru_links:
        # T√¨m t√™n m√¥i gi·ªõi - text trong th·∫ª a ho·∫∑c span con
        name_text = clean_text(link.get_text())
        # Lo·∫°i b·ªè c√°c text kh√¥ng ph·∫£i t√™n
        if name_text and 'Xem th√™m' not in name_text and 'Xem trang' not in name_text and 'Chat' not in name_text and len(name_text) > 2:
            # Ki·ªÉm tra xem c√≥ ph·∫£i t√™n kh√¥ng (2-4 t·ª´, m·ªói t·ª´ b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ hoa)
            # Pattern m·ªÅm h∆°n: ch·∫•p nh·∫≠n c·∫£ t√™n Latin v√† Vi·ªát c√≥ d·∫•u
            # "Don VƒÉn D≈©ng", "Nguy·ªÖn VƒÉn A", "John Smith"
            words = name_text.split()
            if 2 <= len(words) <= 4:
                # M·ªói t·ª´ ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ hoa
                if all(word[0].isupper() or word[0] in 'ƒêƒÇ√Ç√ä√î∆†∆Ø√â√à·∫∫·∫º·∫∏·∫æ·ªÄ·ªÇ·ªÑ·ªÜ·∫§·∫¶·∫®·∫™·∫¨·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª®·ª™·ª¨·ªÆ·ª∞' for word in words if word):
                    # Kh√¥ng ph·∫£i l√† c·ª•m t·ª´ action (Cho Thu, Xem Chi Tiet)
                    if name_text not in ['Cho Thu', 'Cho Thue', 'Xem Chi Tiet', 'Xem Them']:
                        moi_gioi['ten'] = name_text
                        break
    
    # T√¨m s·ªë ƒëi·ªán tho·∫°i
    # 1. T√¨m encrypted phone trong raw_html (attribute "raw")
    encrypted_phone = None
    if raw_html:
        raw_soup = BeautifulSoup(raw_html, 'html.parser')
        for element in raw_soup.find_all(attrs={'raw': True}):
            raw_attr = element.get('raw')
            if raw_attr and len(raw_attr) > 20:  # Encrypted string d√†i > 20 k√Ω t·ª±
                encrypted_phone = raw_attr
                moi_gioi['so_dien_thoai_ma_hoa'] = raw_attr
                break
    
    # 1b. Decrypt phone n·∫øu b·∫≠t
    if encrypted_phone and DECRYPT_PHONE_ENABLED:
        decrypted = decrypt_phone_number(encrypted_phone)
        if decrypted:
            moi_gioi['so_dien_thoai_giai_ma'] = decrypted
            print(f"   ‚úÖ Decrypted: {decrypted}")
    
    # 2. T√¨m s·ªë ƒëi·ªán tho·∫°i hi·ªÉn th·ªã (c√≥ th·ªÉ b·ªã ·∫©n m·ªôt ph·∫ßn)
    # Pattern linh ho·∫°t h∆°n: 0xxx xxx ***, 09xx xxx xxx, etc.
    phone_spans = soup.find_all('span', string=re.compile(r'\d{3,4}\s?\d{3}\s?[\d\*]{3,4}', re.I))
    for span in phone_spans:
        phone_text = clean_text(span.get_text())
        # T√¨m pattern s·ªë ƒëi·ªán tho·∫°i (c√≥ th·ªÉ b·ªã ·∫©n m·ªôt ph·∫ßn)
        phone_match = re.search(r'(\d{3,4}[\s\-]?\d{3}[\s\-]?[\d\*]{3,4})', phone_text)
        if phone_match:
            phone = phone_match.group(1).strip()
            # Chu·∫©n h√≥a format
            phone = re.sub(r'[\s\-]+', ' ', phone)
            moi_gioi['so_dien_thoai'] = phone
            break
    
    # H√¨nh m√¥i gi·ªõi - t√¨m img trong ph·∫ßn c√≥ link guru
    if guru_links:
        for link in guru_links:
            img = link.find('img')
            if img and img.get('src'):
                src = img.get('src')
                if src and not any(word in src.lower() for word in ['logo', 'banner', 'app-store', 'google-play']):
                    if not src.startswith('http'):
                        src = 'https://batdongsan.com.vn' + src
                    moi_gioi['link_hinh'] = src
                    break
    
    if moi_gioi:
        data['moi_gioi'] = moi_gioi
    
    # Images - t√¨m t·∫•t c·∫£ img c√≥ src ch·ª©a "file4.batdongsan.com.vn/resize"
    images = []
    img_tags = soup.find_all('img', src=re.compile(r'file4\.batdongsan\.com\.vn/resize', re.I))
    for img in img_tags:
        src = img.get('src', '')
        if src:
            # Ch·ªâ l·∫•y h√¨nh l·ªõn (1275x717) ho·∫∑c thumbnail (200x200, 255x180)
            if 'resize/1275x717' in src or 'resize/200x200' in src or 'resize/255x180' in src:
                # Lo·∫°i b·ªè logo, banner
                if not any(word in src.lower() for word in ['logo', 'banner', 'app-store', 'google-play', 'footer', 'crop']):
                    if src not in images:
                        images.append(src)
    
    # S·∫Øp x·∫øp: h√¨nh l·ªõn tr∆∞·ªõc
    images.sort(key=lambda x: '1275x717' in x, reverse=True)
    
    if images:
        data['images'] = {
            'album': images,
            'main_image': images[0] if images else None
        }
    
    # T·ªça ƒë·ªô - t√¨m trong nhi·ªÅu ngu·ªìn
    toa_do = {}
    
    # 1. T√¨m trong iframe Google Maps b·∫±ng regex tr·ª±c ti·∫øp tr√™n HTML raw
    # Pattern: data-src="https://www.google.com/maps/embed/v1/place?q=10.786270701503849,106.7317658241369
    iframe_match = re.search(r'(?:data-src|src)="[^"]*google\.com/maps/[^"]*[?&]q=([\d.]+),([\d.]+)', raw_html, re.I)
    if iframe_match:
        toa_do['lat'] = iframe_match.group(1)
        toa_do['lng'] = iframe_match.group(2)
    
    # 2. Backup: t√¨m trong iframe tags parsed (n·∫øu ch∆∞a c√≥)
    if not toa_do:
        iframes = soup.find_all('iframe')
        for iframe in iframes:
            src = iframe.get('data-src', '') or iframe.get('src', '')
            if 'google.com/maps' in src:
                # Pattern: q=20.995635503329915,105.93618149734492
                coord_match = re.search(r'q=([\d.]+),([\d.]+)', src)
                if coord_match:
                    toa_do['lat'] = coord_match.group(1)
                    toa_do['lng'] = coord_match.group(2)
                    break
    
    # 2. T√¨m trong script ho·∫∑c data attribute
    if not toa_do:
        # T√¨m trong script tags c√≥ ch·ª©a t·ªça ƒë·ªô
        scripts = soup.find_all('script')
        for script in scripts:
            script_text = script.string or ''
            # Pattern: lat: 20.995635, lng: 105.93618
            coord_match = re.search(r'lat["\s:]+([0-9.]+).*?lng["\s:]+([0-9.]+)', script_text, re.I | re.DOTALL)
            if coord_match:
                toa_do['lat'] = coord_match.group(1)
                toa_do['lng'] = coord_match.group(2)
                break
            # Pattern kh√°c: [20.995635, 105.93618]
            coord_match = re.search(r'\[([0-9.]{5,}),\s*([0-9.]{5,})\]', script_text)
            if coord_match:
                toa_do['lat'] = coord_match.group(1)
                toa_do['lng'] = coord_match.group(2)
                break
    
    # Fallback 1: T√¨m trong div.place-name
    if not toa_do:
        place_name_elem = soup.select_one('div.place-name')
        if place_name_elem:
            coord_text = clean_text(place_name_elem.get_text())
            # Parse t·ªça ƒë·ªô d·∫°ng "20¬∞59'44.0"N 105¬∞56'10.5"E"
            coord_match = re.search(r'(\d+)¬∞(\d+)\'([\d.]+)"([NS])\s+(\d+)¬∞(\d+)\'([\d.]+)"([EW])', coord_text)
            if coord_match:
                lat_deg = float(coord_match.group(1))
                lat_min = float(coord_match.group(2))
                lat_sec = float(coord_match.group(3))
                lat_dir = coord_match.group(4)
                
                lng_deg = float(coord_match.group(5))
                lng_min = float(coord_match.group(6))
                lng_sec = float(coord_match.group(7))
                lng_dir = coord_match.group(8)
                
                # Convert sang decimal degrees
                lat = lat_deg + lat_min/60 + lat_sec/3600
                if lat_dir == 'S':
                    lat = -lat
                
                lng = lng_deg + lng_min/60 + lng_sec/3600
                if lng_dir == 'W':
                    lng = -lng
                
                toa_do['lat'] = str(lat)
                toa_do['lng'] = str(lng)
                toa_do['raw'] = coord_text
    
    # Fallback 2: T√¨m b·∫•t k·ªÉ text n√†o c√≥ pattern t·ªça ƒë·ªô
    if not toa_do:
        coord_elems = soup.find_all(string=re.compile(r'\d+¬∞\d+\'[\d.]+"[NS]', re.I))
        for elem in coord_elems:
            coord_text = clean_text(elem)
            coord_match = re.search(r'(\d+)¬∞(\d+)\'([\d.]+)"([NS])\s+(\d+)¬∞(\d+)\'([\d.]+)"([EW])', coord_text)
            if coord_match:
                lat_deg = float(coord_match.group(1))
                lat_min = float(coord_match.group(2))
                lat_sec = float(coord_match.group(3))
                lat_dir = coord_match.group(4)
                
                lng_deg = float(coord_match.group(5))
                lng_min = float(coord_match.group(6))
                lng_sec = float(coord_match.group(7))
                lng_dir = coord_match.group(8)
                
                lat = lat_deg + lat_min/60 + lat_sec/3600
                if lat_dir == 'S':
                    lat = -lat
                
                lng = lng_deg + lng_min/60 + lng_sec/3600
                if lng_dir == 'W':
                    lng = -lng
                
                toa_do['lat'] = str(lat)
                toa_do['lng'] = str(lng)
                toa_do['raw'] = coord_text
                break
    
    if toa_do:
        data['toa_do'] = toa_do
    
    return data


async def extract_batdongsan(url: str, output_file: str = None, use_ai: bool = False):
    """
    Extract th√¥ng tin b·∫•t ƒë·ªông s·∫£n v·ªõi ƒë·ªô ch√≠nh x√°c cao
    """
    
    print("=" * 60)
    print("EXTRACT TH√îNG TIN B·∫§T ƒê·ªòNG S·∫¢N")
    print("=" * 60)
    print(f"üìÑ URL: {url}")
    print(f"ü§ñ AI: {'B·∫¨T' if use_ai else 'T·∫ÆT (m·∫∑c ƒë·ªãnh)'}")
    print("=" * 60 + "\n")
    
    async with WebScraper(headless=True, verbose=False) as scraper:
        # B∆∞·ªõc 1: Crawl ƒë·ªÉ l·∫•y HTML v·ªõi js_code ƒë·ªÉ trigger lazy load iframe
        print("üì• B∆∞·ªõc 1: ƒêang crawl trang web...")
        
        # JS code ƒë·ªÉ scroll v√† trigger lazy load iframe
        js_code = [
            "window.scrollTo(0, document.body.scrollHeight);",
            "await new Promise(resolve => setTimeout(resolve, 2000));",
            "const iframes = document.querySelectorAll('iframe[data-src]');",
            "iframes.forEach(iframe => { if (iframe.dataset.src && iframe.dataset.src.includes('google.com/maps')) { iframe.src = iframe.dataset.src; } });",
            "await new Promise(resolve => setTimeout(resolve, 1000));"
        ]
        
        # Retry logic v·ªõi timeout ng·∫Øn h∆°n
        from crawl4ai import CrawlerRunConfig, CacheMode
        max_retries = 3
        retry_count = 0
        raw_result = None
        
        while retry_count < max_retries:
            try:
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    js_code=js_code,
                    page_timeout=30000,  # 30s timeout
                    wait_until="domcontentloaded"  # Kh√¥ng ƒë·ª£i load h·∫øt, ch·ªâ DOM ready
                )
                raw_result = await scraper.crawler.arun(url=url, config=config)
                
                if raw_result.success:
                    break
                    
                retry_count += 1
                if retry_count < max_retries:
                    print(f"‚ö†Ô∏è  Th·ª≠ l·∫°i l·∫ßn {retry_count + 1}/{max_retries}...")
                    await asyncio.sleep(2)
                    
            except Exception as e:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"‚ö†Ô∏è  L·ªói: {str(e)[:100]}. Th·ª≠ l·∫°i l·∫ßn {retry_count + 1}/{max_retries}...")
                    await asyncio.sleep(2)
                else:
                    return {
                        "success": False,
                        "error": f"Connection reset sau {max_retries} l·∫ßn th·ª≠: {str(e)[:200]}"
                    }
        
        if not raw_result or not raw_result.success:
            return {
                "success": False,
                "error": raw_result.error_message if raw_result else "Kh√¥ng crawl ƒë∆∞·ª£c sau 3 l·∫ßn th·ª≠"
            }
        
        raw_html = raw_result.html  # Raw HTML c√≥ iframe
        html = raw_result.cleaned_html or raw_html  # Cleaned HTML ƒë·ªÉ parse
        markdown = raw_result.markdown.raw_markdown if raw_result.markdown else ""
        print("‚úÖ ƒê√£ crawl xong\n")
        
        # B∆∞·ªõc 2: Extract t·ª´ HTML b·∫±ng BeautifulSoup (d√πng cleaned HTML)
        print("üìä B∆∞·ªõc 2: ƒêang extract t·ª´ HTML...")
        html_data = extract_from_html(html, raw_html=raw_html)  # Pass raw_html ƒë·ªÉ extract iframe
        print("‚úÖ HTML extract xong\n")
        
        # B∆∞·ªõc 3: Extract v·ªõi CSS Schema
        print("üìä B∆∞·ªõc 3: ƒêang extract v·ªõi CSS Schema...")
        schema = {
            "name": "BatDongSan",
            "baseSelector": "body",
            "fields": [
                {"name": "title", "selector": "h1.re__pr-title, .re__pr-title, h1", "type": "text"},
                {"name": "price", "selector": ".re__pr-short-info-item-price, .pr-price", "type": "text"},
                {"name": "area", "selector": "[class*='area']", "type": "text"},
                {"name": "address", "selector": ".re__pr-short-info-item-address, .pr-address", "type": "text"},
                {"name": "description", "selector": ".re__section-body, .pr-description", "type": "text"},
            ]
        }
        
        schema_result = await scraper.scrape_with_schema(url, schema, bypass_cache=True)
        schema_data = {}
        if schema_result.get("extracted_data"):
            if isinstance(schema_result["extracted_data"], list) and len(schema_result["extracted_data"]) > 0:
                schema_data = schema_result["extracted_data"][0]
            elif isinstance(schema_result["extracted_data"], dict):
                schema_data = schema_result["extracted_data"]
        
        # Merge schema data v√†o html_data
        if schema_data.get("title"):
            html_data["title"] = clean_text(schema_data["title"])
        if schema_data.get("price"):
            html_data["khoang_gia"] = clean_text(schema_data["price"])
        if schema_data.get("area"):
            html_data["dien_tich"] = clean_text(schema_data["area"])
        if schema_data.get("address"):
            html_data["dia_chi"] = clean_text(schema_data["address"])
        if schema_data.get("description"):
            html_data["mo_ta"] = clean_text(schema_data["description"])
        
        print("‚úÖ CSS Schema extract xong\n")
        
        # B∆∞·ªõc 4: Extract v·ªõi AI ƒë·ªÉ b·ªï sung (n·∫øu b·∫≠t)
        ai_data = {}
        
        if use_ai:
            print("ü§ñ B∆∞·ªõc 4: ƒêang extract v·ªõi AI ƒë·ªÉ b·ªï sung...")
            api_key = os.getenv("GROQ_API_KEY")
            if api_key:
                try:
                    llm_config = LLMConfig(
                        provider="groq/llama-3.3-70b-versatile",
                        api_token=api_key
                    )
                    
                    instruction = """
                    B·ªï sung c√°c th√¥ng tin c√≤n thi·∫øu ho·∫∑c ch∆∞a ch√≠nh x√°c:
                    - T√™n d·ª± √°n ƒë·∫ßy ƒë·ªß (v√≠ d·ª•: "The Paris - Vinhomes Ocean Park", kh√¥ng ph·∫£i slug)
                    - ƒê·ªãa ch·ªâ ƒë·∫ßy ƒë·ªß (kh√¥ng c√≥ markdown link)
                    - ƒê·∫∑c ƒëi·ªÉm b·∫•t ƒë·ªông s·∫£n (h∆∞·ªõng ban c√¥ng, ƒë∆∞·ªùng v√†o, ph√°p l√Ω)
                    - Th√¥ng tin d·ª± √°n
                    - T·ªça ƒë·ªô n·∫øu c√≥
                    - T√™n m√¥i gi·ªõi ch√≠nh x√°c (KH√îNG ph·∫£i "Cho Thu")
                    - S·ªë ƒëi·ªán tho·∫°i (c√≥ th·ªÉ b·ªã ·∫©n m·ªôt ph·∫ßn)
                    
                    Tr·∫£ v·ªÅ JSON ch·ªâ v·ªõi c√°c th√¥ng tin B·ªî SUNG ho·∫∑c S·ª¨A L·∫†I.
                    """
                    
                    ai_result = await scraper.scrape_with_llm(url, instruction, llm_config, bypass_cache=True)
                    
                    if ai_result.get("success") and ai_result.get("extracted_data"):
                        ai_data_raw = ai_result["extracted_data"]
                        if isinstance(ai_data_raw, list) and len(ai_data_raw) > 0:
                            ai_data = ai_data_raw[0]
                        elif isinstance(ai_data_raw, dict):
                            ai_data = ai_data_raw
                    
                    print("‚úÖ AI extract xong\n")
                except Exception as e:
                    print(f"‚ö†Ô∏è AI l·ªói: {e}, b·ªè qua\n")
        else:
            print("‚è≠Ô∏è B·ªè qua AI extraction\n")
        
        # B∆∞·ªõc 5: Merge t·∫•t c·∫£ d·ªØ li·ªáu (∆∞u ti√™n HTML > Schema > AI)
        final_data = {
            "title": clean_text(html_data.get("title") or ai_data.get("title", "")),
            "images": html_data.get("images") or ai_data.get("images", {"album": [], "main_image": None}),
            "du_an": html_data.get("du_an") or ai_data.get("du_an", {}),
            "dia_chi": html_data.get("dia_chi") or ai_data.get("dia_chi", ""),
            "mo_ta": html_data.get("mo_ta") or ai_data.get("mo_ta", ""),
            "dac_diem": html_data.get("dac_diem") or ai_data.get("dac_diem", {}),
            "thong_tin_du_an": ai_data.get("thong_tin_du_an", {}),
            "toa_do": html_data.get("toa_do") or ai_data.get("toa_do", {}),
            "ngay_dang": html_data.get("ngay_dang") or ai_data.get("ngay_dang", ""),
            "ngay_het_han": html_data.get("ngay_het_han") or ai_data.get("ngay_het_han", ""),
            "loai_tin": html_data.get("loai_tin") or ai_data.get("loai_tin", ""),
            "ma_tin": html_data.get("ma_tin") or "",
            "duan_id": ai_data.get("duan_id", ""),
            "moi_gioi": html_data.get("moi_gioi") or ai_data.get("moi_gioi", {})
        }
        
        # Extract m√£ tin t·ª´ URL (n·∫øu ch∆∞a c√≥ t·ª´ HTML)
        if not final_data.get("ma_tin"):
            ma_tin_match = re.search(r'pr(\d+)', url)
            if ma_tin_match:
                final_data["ma_tin"] = ma_tin_match.group(1)
        
        # Clean title n·∫øu v·∫´n c√≥ markdown
        if final_data["title"] and ('[' in final_data["title"] or '](' in final_data["title"]):
            final_data["title"] = clean_text(final_data["title"])
        
        # Clean ƒë·ªãa ch·ªâ n·∫øu v·∫´n c√≥ markdown
        if final_data["dia_chi"] and '](' in final_data["dia_chi"]:
            final_data["dia_chi"] = clean_text(final_data["dia_chi"])
        
        # Clean t√™n d·ª± √°n
        if final_data.get("du_an", {}).get("ten"):
            ten_du_an = final_data["du_an"]["ten"]
            # Ki·ªÉm tra xem c√≥ ph·∫£i slug kh√¥ng (v√≠ d·ª•: "the-paris-vinhomes-ocean-park")
            if '-' in ten_du_an and ten_du_an.islower():
                # L√† slug, t√¨m t√™n th·∫≠t t·ª´ AI
                if ai_data.get("du_an", {}).get("ten"):
                    final_data["du_an"]["ten"] = ai_data["du_an"]["ten"]
                else:
                    # Convert slug th√†nh t√™n d·ª± √°n (capitalize m·ªói t·ª´)
                    final_data["du_an"]["ten"] = ' '.join(word.capitalize() for word in ten_du_an.split('-'))
            elif ')' in ten_du_an and '(' not in ten_du_an:
                # C√≥ th·ªÉ l√† slug kh√°c, t√¨m t√™n th·∫≠t t·ª´ AI
                if ai_data.get("du_an", {}).get("ten"):
                    final_data["du_an"]["ten"] = ai_data["du_an"]["ten"]
            final_data["du_an"]["ten"] = clean_text(final_data["du_an"]["ten"])
        
        # Validate v√† clean m√¥i gi·ªõi
        if final_data.get("moi_gioi", {}).get("ten"):
            ten_mg = final_data["moi_gioi"]["ten"]
            # Lo·∫°i b·ªè c√°c t√™n kh√¥ng h·ª£p l·ªá
            invalid_names = ['Cho Thu', 'Cho Thue', 'Xem Chi Tiet', 'Xem Them', 'Chat Ngay']
            if ten_mg in invalid_names:
                # Th·ª≠ l·∫•y t·ª´ AI
                if ai_data.get("moi_gioi", {}).get("ten"):
                    final_data["moi_gioi"]["ten"] = ai_data["moi_gioi"]["ten"]
                else:
                    # X√≥a t√™n kh√¥ng h·ª£p l·ªá
                    del final_data["moi_gioi"]["ten"]
        
        result = {
            "success": True,
            "url": url,
            "extracted_at": datetime.now().isoformat(),
            "data": final_data
        }
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£ JSON ngay
        print("\n" + "=" * 60)
        print("üìä JSON OUTPUT")
        print("=" * 60)
        print(json.dumps(final_data, indent=2, ensure_ascii=False))
        print("=" * 60 + "\n")
        
        # L∆∞u file
        if output_file:
            # N·∫øu output_file kh√¥ng c√≥ path, th√™m v√†o output/
            if '/' not in output_file and '\\' not in output_file:
                output_file = f"output/{output_file}"
            # N·∫øu ƒë√£ c√≥ output/ r·ªìi th√¨ kh√¥ng th√™m n·ªØa
            elif output_file.startswith('output/'):
                pass
            scraper.save_to_file(result, output_file, output_dir="")
            print(f"üíæ ƒê√£ l∆∞u v√†o: {output_file}")
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"batdongsan_{timestamp}.json"
            scraper.save_to_file(result, filename)
            print(f"üíæ ƒê√£ l∆∞u v√†o: output/{filename}")
        
        return result


async def main():
    if len(sys.argv) < 2:
        print("=" * 60)
        print(" TOOL EXTRACT TH√îNG TIN B·∫§T ƒê·ªòNG S·∫¢N")
        print("=" * 60)
        print("\nC√°ch s·ª≠ d·ª•ng:")
        print("  python extract_batdongsan.py <URL> [output_file] [--ai]")
        print("\nV√≠ d·ª•:")
        print("  python extract_batdongsan.py <URL>                    # Kh√¥ng AI (m·∫∑c ƒë·ªãnh)")
        print("  python extract_batdongsan.py <URL> --ai               # C√≥ AI")
        print("  python extract_batdongsan.py <URL> result.json        # Kh√¥ng AI + file output")
        print("  python extract_batdongsan.py <URL> result.json --ai   # C√≥ AI + file output")
        print("\n" + "=" * 60)
        return
    
    url = sys.argv[1]
    output_file = None
    use_ai = False
    
    # Parse arguments
    for arg in sys.argv[2:]:
        if arg == "--ai":
            use_ai = True
        elif not arg.startswith("--"):
            output_file = arg
    
    result = await extract_batdongsan(url, output_file, use_ai)
    
    if result["success"]:
        print("\n" + "=" * 60)
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("=" * 60)
    else:
        print(f"\n‚ùå L·ªói: {result.get('error')}")


if __name__ == "__main__":
    # Fix encoding cho Windows console
    import sys
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    asyncio.run(main())

