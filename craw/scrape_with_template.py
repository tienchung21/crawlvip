"""
Script Ä‘Æ¡n giáº£n Ä‘á»ƒ cÃ o dá»¯ liá»‡u sá»­ dá»¥ng template Ä‘Ã£ lÆ°u tá»« extension

CÃ¡ch sá»­ dá»¥ng:
    python scrape_with_template.py <template_file> <url> [output_file]

VÃ­ dá»¥:
    python scrape_with_template.py crawl4ai_template_1234567890.json https://batdongsan.com.vn/... output.json
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from web_scraper import WebScraper


async def scrape_with_template(template_path: str, url: str, output_file: str = None):
    """
    CÃ o dá»¯ liá»‡u sá»­ dá»¥ng template tá»« extension
    
    Args:
        template_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file template JSON
        url: URL cáº§n cÃ o
        output_file: File Ä‘á»ƒ lÆ°u káº¿t quáº£ (náº¿u None sáº½ tá»± Ä‘á»™ng táº¡o tÃªn)
    """
    # Kiá»ƒm tra file template
    if not Path(template_path).exists():
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file template: {template_path}")
        return None
    
    # Äá»c template
    print(f"ğŸ“‹ Äang Ä‘á»c template: {template_path}")
    with open(template_path, 'r', encoding='utf-8') as f:
        template = json.load(f)
    
    print(f"ğŸ“„ Template: {template.get('name', 'Unknown')}")
    print(f"ğŸ“… Táº¡o lÃºc: {template.get('createdAt', 'Unknown')}")
    print(f"ğŸ”¢ Sá»‘ trÆ°á»ng: {len(template.get('fields', []))}")
    print(f"ğŸŒ URL gá»‘c: {template.get('url', 'Unknown')}")
    print(f"ğŸ¯ URL cáº§n cÃ o: {url}\n")
    
    # Táº¡o schema cho Crawl4AI
    schema = {
        "name": template.get("name", "ExtractedData"),
        "baseSelector": template.get("baseSelector") or "body",
        "fields": []
    }
    
    # Chuyá»ƒn Ä‘á»•i fields tá»« template sang format Crawl4AI
    for field in template.get("fields", []):
        field_config = {
            "name": field["name"],
            "selector": field["selector"],
            "type": field.get("type", "text")
        }
        
        # ThÃªm attribute náº¿u cÃ³
        if field.get("attribute"):
            field_config["attribute"] = field["attribute"]
        
        schema["fields"].append(field_config)
    
    print("ğŸ“Š Schema Ä‘Ã£ táº¡o:")
    print(json.dumps(schema, indent=2, ensure_ascii=False))
    print("\n" + "="*60 + "\n")
    
    # CÃ o dá»¯ liá»‡u vá»›i Crawl4AI
    print("ğŸš€ Äang cÃ o dá»¯ liá»‡u vá»›i Crawl4AI...")
    async with WebScraper(headless=True, verbose=False) as scraper:
        result = await scraper.scrape_with_schema(url, schema, bypass_cache=True)
        
        if result["success"]:
            print("âœ… CÃ o thÃ nh cÃ´ng!\n")
            
            # Xá»­ lÃ½ extracted_data
            extracted_data = result.get("extracted_data", {})
            if isinstance(extracted_data, list) and len(extracted_data) > 0:
                extracted_data = extracted_data[0]
            
            print("ğŸ“Š Dá»¯ liá»‡u Ä‘Ã£ extract:")
            print(json.dumps(extracted_data, indent=2, ensure_ascii=False))
            
            # LÆ°u káº¿t quáº£
            if output_file is None:
                # Táº¡o tÃªn file tá»± Ä‘á»™ng
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"output/scraped_{timestamp}.json"
            
            # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # LÆ°u file
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "url": url,
                    "template": template_path,
                    "template_name": template.get("name"),
                    "scraped_at": datetime.now().isoformat(),
                    "data": extracted_data
                }, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o: {output_file}")
            return extracted_data
        else:
            print(f"âŒ Lá»—i: {result.get('error', 'Unknown error')}")
            return None


async def main():
    """Main function"""
    if len(sys.argv) < 3:
        print("ğŸ“– CÃ¡ch sá»­ dá»¥ng:")
        print(f"   python {sys.argv[0]} <template_file> <url> [output_file]")
        print("\nğŸ’¡ VÃ­ dá»¥:")
        print(f"   python {sys.argv[0]} crawl4ai_template_1234567890.json https://batdongsan.com.vn/...")
        print(f"   python {sys.argv[0]} template.json https://example.com output.json")
        sys.exit(1)
    
    template_path = sys.argv[1]
    url = sys.argv[2]
    output_file = sys.argv[3] if len(sys.argv) > 3 else None
    
    await scrape_with_template(template_path, url, output_file)


if __name__ == "__main__":
    asyncio.run(main())

